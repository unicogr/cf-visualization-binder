{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392f6d62",
   "metadata": {},
   "source": [
    "# Whole-brain pRF and CF maps\n",
    "\n",
    "Here you can interactively visualize both population Receptive Field (**pRF**)and Connective Field (**CF**) maps on the cortical surface.\n",
    "\n",
    "\n",
    "**pRF** maps characterize the aggregate response properties of neuronal populations in visual cortex to stimuli in visual space. pRF parameters include eccentricity (distance from fixation), polar angle (angular position in the visual field), and size (extent of the receptive field), providing a functional description of retinotopic organization within individual visual areas.\n",
    "\n",
    "\n",
    "**CF** maps reveal functional connectivity patterns across the brain. You can select CF parameters (e.g., eccentricity or polar angle) for models referred to left or right V1. Target areas for V1 connectivity include the whole brain, allowing CFs in V1 to project to the contralateral hemisphere, potentially revealing retinotopically organized connectivity in contralateral V1, as well as elsewhere in the cortex.\n",
    "\n",
    "Use the widgets below to switch between CF and pRF maps, select different parameters, and adjust visualization settings.\n",
    "\n",
    "*Nicolas Gravel | CEA | nicolas.gravel (at) cea.fr | 28-01-2025*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Palette Definitions (Updated)\n",
    "# Safe imports with error handling\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core imports\n",
    "try:\n",
    "    import os\n",
    "    import glob\n",
    "    import yaml\n",
    "    from pathlib import Path\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "    import re\n",
    "except Exception as e:\n",
    "    print(f'❌ Core imports failed: {e}')\n",
    "    raise\n",
    "\n",
    "# Neuroimaging imports\n",
    "try:\n",
    "    import neuropythy as ny\n",
    "    from neuropythy.geometry import Mesh, Tesselation\n",
    "except Exception as e:\n",
    "    print(f'❌ Neuropythy failed: {e}')\n",
    "    raise\n",
    "\n",
    "# Visualization imports\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import ipyvolume as ipv\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.patches import Patch, Wedge\n",
    "    import matplotlib.colors as mcolors\n",
    "    from matplotlib import cm\n",
    "    from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "except Exception as e:\n",
    "    print(f'❌ Visualization imports failed: {e}')\n",
    "    raise\n",
    "\n",
    "# Additional scientific imports\n",
    "try:\n",
    "    from nilearn.surface import vol_to_surf\n",
    "    from nilearn import surface, plotting, signal\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    from scipy.spatial import cKDTree\n",
    "    from scipy.stats import pearsonr\n",
    "except Exception as e:\n",
    "    pass  # Optional imports, silent fail\n",
    "\n",
    "# Widget imports\n",
    "try:\n",
    "    from ipywidgets import FloatText, HBox, VBox, Textarea, Output, Dropdown, FloatSlider, interactive_output\n",
    "    from traitlets import link\n",
    "    from IPython.display import display, HTML\n",
    "except Exception as e:\n",
    "    print(f'❌ Widget imports failed: {e}')\n",
    "    raise\n",
    "\n",
    "import math\n",
    "import gc\n",
    "\n",
    "# Debug flag - set to True to see detailed progress messages\n",
    "DEBUG = False\n",
    "\n",
    "# Inline color palettes (no external cfmap dependency)\n",
    "def get_eccentricity_palette():\n",
    "    \"\"\"\n",
    "    Returns a color palette with 10 colors transitioning from red to orange to yellow to green to turquoise to cyan to blue.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing different formats of the color palette\n",
    "    \"\"\"\n",
    "    from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # Original RGB values (0-255)\n",
    "    rgb_values = [\n",
    "        [255, 40, 0],    # Red\n",
    "        [255, 130, 0],   # Orange-red\n",
    "        [255, 210, 0],   # Orange-yellow\n",
    "        [255, 255, 0],   # Yellow\n",
    "        [115, 255, 0],   # Yellow-green\n",
    "        [31, 255, 0],    # Green\n",
    "        [0, 255, 207],   # Turquoise\n",
    "        [0, 231, 255],   # Cyan\n",
    "        [20, 140, 255],  # Light blue\n",
    "        [40, 60, 255]    # Blue\n",
    "    ]\n",
    "    \n",
    "    # Normalize to 0-1 range for matplotlib\n",
    "    norm_values = [[r/255, g/255, b/255] for r, g, b in rgb_values]\n",
    "    \n",
    "    # Create hex values\n",
    "    hex_values = [mcolors.rgb2hex(rgb) for rgb in norm_values]\n",
    "    \n",
    "    # Create named colors\n",
    "    named_colors = {f\"color{i+1}\": hex_values[i] for i in range(len(hex_values))}\n",
    "    \n",
    "    return {\n",
    "        \"rgb_0_255\": rgb_values,\n",
    "        \"rgb_0_1\": norm_values,\n",
    "        \"hex\": hex_values,\n",
    "        \"named\": named_colors,\n",
    "        \"matplotlib_cmap\": LinearSegmentedColormap.from_list(\"eccen_cmap\", norm_values)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_polar_palette():\n",
    "    \"\"\"\n",
    "    Returns a color palette with 20 colors transitioning from green to red to green to blue to green.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing different formats of the color palette\n",
    "    \"\"\"\n",
    "    from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # Original RGB values (0-255)\n",
    "    rgb_values = [\n",
    "        [106, 189, 69],   # Color1\n",
    "        [203, 219, 42],   # Color2\n",
    "        [254, 205, 8],    # Color3\n",
    "        [242, 104, 34],   # Color4\n",
    "        [237, 32, 36],    # Color5\n",
    "        [237, 32, 36],    # Color6\n",
    "        [242, 104, 34],   # Color7\n",
    "        [254, 205, 8],    # Color8\n",
    "        [203, 219, 42],   # Color9\n",
    "        [106, 189, 69],   # Color10\n",
    "        [106, 189, 69],   # Color11\n",
    "        [110, 205, 221],  # Color12\n",
    "        [50, 178, 219],   # Color13\n",
    "        [62, 105, 179],   # Color14\n",
    "        [57, 84, 165],    # Color15\n",
    "        [57, 84, 165],    # Color16\n",
    "        [62, 105, 179],   # Color17\n",
    "        [50, 178, 219],   # Color18\n",
    "        [110, 205, 221],  # Color19\n",
    "        [106, 189, 69]    # Color20\n",
    "    ]\n",
    "    \n",
    "    # Normalize to 0-1 range for matplotlib\n",
    "    norm_values = [[r/255, g/255, b/255] for r, g, b in rgb_values]\n",
    "    \n",
    "    # Create hex values\n",
    "    hex_values = [mcolors.rgb2hex(rgb) for rgb in norm_values]\n",
    "    \n",
    "    # Create named colors\n",
    "    named_colors = {f\"color{i+1}\": hex_values[i] for i in range(len(hex_values))}\n",
    "    \n",
    "    return {\n",
    "        \"rgb_0_255\": rgb_values,\n",
    "        \"rgb_0_1\": norm_values,\n",
    "        \"hex\": hex_values,\n",
    "        \"named\": named_colors,\n",
    "        \"matplotlib_cmap\": LinearSegmentedColormap.from_list(\"polar_cmap\", norm_values)\n",
    "    }\n",
    "\n",
    "\n",
    "# Get color palettes\n",
    "eccen_colors = get_eccentricity_palette()\n",
    "polar_colors = get_polar_palette()\n",
    "\n",
    "\n",
    "# Rotate axis\n",
    "def rotate_coords(coords, axis, angle_degrees):\n",
    "    \"\"\"\n",
    "    Rotates coordinates by a given angle around the specified axis.\n",
    "    \n",
    "    Parameters:\n",
    "        coords (np.ndarray): shape (3, N) (x, y, z as first dimension)\n",
    "        axis (str): 'x', 'y', or 'z'\n",
    "        angle_degrees (float): rotation angle in degrees\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: rotated coordinates, shape (3, N)\n",
    "        axis (str): 'x', 'y', or 'z'\n",
    "    \"\"\"    \n",
    "    theta = np.deg2rad(angle_degrees)\n",
    "    if axis == 'x':\n",
    "        rot = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, np.cos(theta), -np.sin(theta)],\n",
    "            [0, np.sin(theta),  np.cos(theta)]\n",
    "        ])\n",
    "    elif axis == 'y':\n",
    "        rot = np.array([\n",
    "            [ np.cos(theta), 0, np.sin(theta)],\n",
    "            [ 0,             1, 0            ],\n",
    "            [-np.sin(theta), 0, np.cos(theta)]\n",
    "        ])\n",
    "    elif axis == 'z':\n",
    "        rot = np.array([\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta),  np.cos(theta), 0],\n",
    "            [0,              0,             1]\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"axis must be 'x', 'y', or 'z'\")\n",
    "    return rot @ coords\n",
    "\n",
    "\n",
    "# Improved state management class\n",
    "class CFVisualizationState:\n",
    "    \"\"\"\n",
    "    Manages the state of the visualization, including caches for data and intermediate results.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.notebook_dir = Path().resolve()\n",
    "        self.loaded_results_cache = {}\n",
    "        self.loaded_mesh_cache = {}\n",
    "        # Cache for processed data (masked, transformed, etc.) based on parameters\n",
    "        # Key: (dataset_key, subject_id, task, source_hemi, map_type, cf_property, r2_threshold)\n",
    "        self.processed_data_cache = {}\n",
    "        # Cache for transformed meshes (rotated, separated) based on separation\n",
    "        # Key: (dataset_key, subject_id, hemi_separation)\n",
    "        self.transformed_mesh_cache = {}\n",
    "        # Keep track of the current plot object for potential updates (though full redraw is common)\n",
    "        self.current_fig = None\n",
    "        self.current_plot_data = {} # Store latest plot params and data\n",
    "\n",
    "    def get_paths_and_configs(self, dataset_key):\n",
    "        \"\"\"Helper to get paths based on current dataset.\"\"\"\n",
    "        base_data_path = self.notebook_dir / 'data' / dataset_key\n",
    "        cf_models_dir = base_data_path / 'derivatives' / 'cf-models'\n",
    "        fs_subjects_dir = base_data_path / 'fs_subjects'\n",
    "        return base_data_path, cf_models_dir, fs_subjects_dir\n",
    "\n",
    "    def load_meshes_and_curv(self, fs_subject_path, cache_key):\n",
    "        \"\"\"Loads or retrieves cached meshes and curvature maps.\"\"\"\n",
    "        if cache_key in self.loaded_mesh_cache:\n",
    "            if DEBUG:\n",
    "                print(f\"✓ Using cached mesh data for {cache_key}\")\n",
    "            return self.loaded_mesh_cache[cache_key]\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(f\"⏳ Loading mesh from disk: {fs_subject_path}\")\n",
    "        try:\n",
    "            import nibabel.freesurfer as fs\n",
    "            surf_dir = fs_subject_path / 'surf'\n",
    "            \n",
    "            # Load geometry\n",
    "            lh_coords, lh_faces = fs.read_geometry(str(surf_dir / 'lh.inflated'))\n",
    "            rh_coords, rh_faces = fs.read_geometry(str(surf_dir / 'rh.inflated'))\n",
    "            \n",
    "            # Create neuropythy Mesh objects\n",
    "            lh_mesh = Mesh(Tesselation(lh_faces.T), lh_coords.T)\n",
    "            rh_mesh = Mesh(Tesselation(rh_faces.T), rh_coords.T)\n",
    "            \n",
    "            # Load curvature maps\n",
    "            lh_curv_map = fs.read_morph_data(str(surf_dir / 'lh.curv'))\n",
    "            rh_curv_map = fs.read_morph_data(str(surf_dir / 'rh.curv'))\n",
    "\n",
    "            mesh_data = {\n",
    "                'lh_mesh': lh_mesh,\n",
    "                'rh_mesh': rh_mesh,\n",
    "                'lh_curv_map': lh_curv_map,\n",
    "                'rh_curv_map': rh_curv_map\n",
    "            }\n",
    "            self.loaded_mesh_cache[cache_key] = mesh_data\n",
    "            if DEBUG:\n",
    "                print(f\"✓ Loaded and cached mesh: LH={lh_coords.shape[0]}, RH={rh_coords.shape[0]} vertices\")\n",
    "            return mesh_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading mesh: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_cf_results(self, subject_id, task, source_hemi, dataset_key):\n",
    "        \"\"\"Loads or retrieves cached CF results.\"\"\"\n",
    "        cache_key = f\"{dataset_key}_sub-{subject_id}_{task}_{source_hemi}\"\n",
    "        \n",
    "        if cache_key in self.loaded_results_cache:\n",
    "            if DEBUG:\n",
    "                print(f\"Using cached CF results for {cache_key}\")\n",
    "            return self.loaded_results_cache[cache_key]\n",
    "        \n",
    "        # Find matching file (requires cf_models_info to be accessible)\n",
    "        # Assume cf_models_info is available in the scope where this is called or passed\n",
    "        matching_files = [m for m in cf_models_info \n",
    "                         if m['subject'] == subject_id \n",
    "                         and m['task'] == task \n",
    "                         and m['source_hemi'] == source_hemi]\n",
    "        \n",
    "        if not matching_files:\n",
    "            print(f\"❌ No CF model found for sub-{subject_id}, task={task}, source={source_hemi}\")\n",
    "            return None, None\n",
    "        \n",
    "        model_file = matching_files[0]['file']\n",
    "        if DEBUG:\n",
    "            print(f\"Loading CF results from: {model_file}\")\n",
    "            \n",
    "        # Load results\n",
    "        results = np.load(model_file, allow_pickle=True)\n",
    "        results_lh = results['lh_results'].item()\n",
    "        results_rh = results['rh_results'].item()\n",
    "        \n",
    "        # Cache results\n",
    "        self.loaded_results_cache[cache_key] = (results_lh, results_rh)\n",
    "        #print(f\"✓ Loaded and cached CF results for {cache_key}\")\n",
    "        \n",
    "        return results_lh, results_rh\n",
    "\n",
    "   \n",
    "    def load_prf_results(self, subject_id, dataset_key):\n",
    "        \"\"\"Loads or retrieves cached pRF results.\"\"\"\n",
    "        cache_key = f\"prf_{dataset_key}_sub-{subject_id}\"\n",
    "\n",
    "        #if cache_key in self.loaded_results_cache:\n",
    "        #    print(f\"Using cached pRF results for {cache_key}\")\n",
    "        #    return self.loaded_results_cache[cache_key]\n",
    "\n",
    "        dataset_root = self.notebook_dir / 'data' / dataset_key\n",
    "        prf_file = dataset_root / 'derivatives' / 'pRF-maps' / f'pRF_parameters_both_hemispheres_averaged_sub-{subject_id}_ses-01.csv'\n",
    "\n",
    "        empty_lh = {\n",
    "            'polar_angle': [], 'eccentricity': [], 'size': [], 'baseline': [],\n",
    "            'amplitude': [], 'hrf_delay': [], 'hrf_dispersion': [], 'r2': []\n",
    "        }\n",
    "        empty_rh = { k: [] for k in empty_lh.keys() }\n",
    "\n",
    "        if not prf_file.exists():\n",
    "            print(f\"⚠️ pRF file not found: {prf_file}\")\n",
    "            self.loaded_results_cache[cache_key] = (empty_lh, empty_rh)\n",
    "            return empty_lh, empty_rh\n",
    "\n",
    "        #print(f\"Loading pRF results from: {prf_file}\")\n",
    "        df = pd.read_csv(prf_file, sep=',', engine='python')\n",
    "        \n",
    "        cols_lower = {c.lower(): c for c in df.columns}\n",
    "        hemi_col = cols_lower.get('hemisphere') or cols_lower.get('hemi')\n",
    "        vertex_col = cols_lower.get('vertex_index') or cols_lower.get('vertex')\n",
    "        pa_col = cols_lower.get('polar_angle') or cols_lower.get('polar')\n",
    "        ecc_col = cols_lower.get('eccentricity')\n",
    "        size_col = cols_lower.get('sd')\n",
    "        r2_col = cols_lower.get('r2')\n",
    "        baseline_col = cols_lower.get('baseline')\n",
    "        amplitude_col = cols_lower.get('amplitude')\n",
    "        hrf_delay_col = cols_lower.get('hrf_delay')\n",
    "        hrf_dispersion_col = cols_lower.get('hrf_dispersion') # Fixed typo\n",
    "\n",
    "        if hemi_col is None:\n",
    "            print(f\"⚠️ pRF CSV missing 'hemisphere' column: {prf_file.name}\")\n",
    "            self.loaded_results_cache[cache_key] = (empty_lh, empty_rh)\n",
    "            return empty_lh, empty_rh\n",
    "\n",
    "        lh_results = { k: [] for k in empty_lh.keys() }\n",
    "        rh_results = { k: [] for k in empty_lh.keys() }\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            hemi = str(row[hemi_col]).lower()\n",
    "            try:\n",
    "                vidx = int(row[vertex_col])\n",
    "            except Exception:\n",
    "                continue\n",
    "            hemi = str(row[hemi_col]).lower()\n",
    "            target_dict = lh_results if hemi.startswith('lh') else rh_results if hemi.startswith('rh') else None\n",
    "            if target_dict is None:\n",
    "                continue\n",
    "\n",
    "            if pa_col and pa_col in row and not pd.isna(row[pa_col]):\n",
    "                target_dict['polar_angle'].append((vidx, row[pa_col]))\n",
    "            if ecc_col and ecc_col in row and not pd.isna(row[ecc_col]):\n",
    "                target_dict['eccentricity'].append((vidx, row[ecc_col]))\n",
    "            if size_col and size_col in row and not pd.isna(row[size_col]):\n",
    "                target_dict['size'].append((vidx, row[size_col]))\n",
    "            if r2_col and r2_col in row and not pd.isna(row[r2_col]):\n",
    "                target_dict['r2'].append((vidx, row[r2_col]))\n",
    "            if baseline_col and baseline_col in row and not pd.isna(row[baseline_col]):\n",
    "                target_dict['baseline'].append((vidx, row[baseline_col]))\n",
    "            if amplitude_col and amplitude_col in row and not pd.isna(row[amplitude_col]):\n",
    "                target_dict['amplitude'].append((vidx, row[amplitude_col]))\n",
    "            if hrf_delay_col and hrf_delay_col in row and not pd.isna(row[hrf_delay_col]):\n",
    "                target_dict['hrf_delay'].append((vidx, row[hrf_delay_col]))\n",
    "            if hrf_dispersion_col and hrf_dispersion_col in row and not pd.isna(row[hrf_dispersion_col]): # Use corrected name\n",
    "                target_dict['hrf_dispersion'].append((vidx, row[hrf_dispersion_col]))\n",
    "\n",
    "        self.loaded_results_cache[cache_key] = (lh_results, rh_results)\n",
    "        #print(f\"✓ Loaded and cached pRF results for {cache_key}\")\n",
    "        return lh_results, rh_results\n",
    "\n",
    "    def prepare_prf_data_arrays(self, lh_results_dict, rh_results_dict, lh_mesh_shape, rh_mesh_shape, cf_property):\n",
    "        \"\"\"Converts pRF dictionary results to full numpy arrays for plotting.\"\"\"\n",
    "        # Initialize arrays with NaNs\n",
    "        lh_data = np.full(lh_mesh_shape[1], np.nan) # Shape is (3, n_vertices)\n",
    "        rh_data = np.full(rh_mesh_shape[1], np.nan)\n",
    "        lh_r2 = np.full(lh_mesh_shape[1], np.nan)\n",
    "        rh_r2 = np.full(rh_mesh_shape[1], np.nan)\n",
    "\n",
    "        # Fill data arrays from dictionaries\n",
    "        # Left Hemisphere\n",
    "        for vidx, val in lh_results_dict[cf_property]:\n",
    "            if 0 <= vidx < lh_data.shape[0]:\n",
    "                if cf_property == 'size':\n",
    "                    lh_data[vidx] = abs(val) * 2.355\n",
    "                else:\n",
    "                    lh_data[vidx] = val\n",
    "        for vidx, val in lh_results_dict['r2']:\n",
    "            if 0 <= vidx < lh_r2.shape[0]:\n",
    "                lh_r2[vidx] = val\n",
    "\n",
    "        # Right Hemisphere\n",
    "        for vidx, val in rh_results_dict[cf_property]:\n",
    "            if 0 <= vidx < rh_data.shape[0]:\n",
    "                if cf_property == 'size':\n",
    "                    rh_data[vidx] = abs(val) * 2.355\n",
    "                else:\n",
    "                    rh_data[vidx] = val\n",
    "        for vidx, val in rh_results_dict['r2']:\n",
    "            if 0 <= vidx < rh_r2.shape[0]:\n",
    "                rh_r2[vidx] = val\n",
    "                \n",
    "        return lh_data, rh_data, lh_r2, rh_r2\n",
    "\n",
    "    def apply_r2_mask(self, lh_data, rh_data, lh_r2, rh_r2, r2_threshold):\n",
    "        \"\"\"Applies R2 threshold mask to data arrays.\"\"\"\n",
    "        lh_mask = np.isnan(lh_r2) | (lh_r2 < r2_threshold)\n",
    "        rh_mask = np.isnan(rh_r2) | (rh_r2 < r2_threshold)\n",
    "        \n",
    "        # Apply mask by setting values to NaN\n",
    "        lh_data_masked = lh_data.copy()\n",
    "        rh_data_masked = rh_data.copy()\n",
    "        lh_data_masked[lh_mask] = np.nan\n",
    "        rh_data_masked[rh_mask] = np.nan\n",
    "        \n",
    "        return lh_data_masked, rh_data_masked, lh_mask, rh_mask\n",
    "\n",
    "    def get_property_specific_config(self, cf_property, map_type):\n",
    "        \"\"\"Gets default configuration for a given property.\"\"\"\n",
    "        property_config = {\n",
    "            'r2': {'adaptive': True, 'vmin': 0, 'vmax': 1},\n",
    "            'polar': {'adaptive': True, 'vmin': -np.pi, 'vmax': np.pi},\n",
    "            'eccentricity': {'adaptive': False, 'vmin': 0.5, 'vmax': 6.5},\n",
    "            'cf_size': {'adaptive': False, 'vmin': 0.5, 'vmax': 5.0},\n",
    "            'polar_angle': {'adaptive': True, 'vmin': -np.pi, 'vmax': np.pi},\n",
    "            'size': {'adaptive': False, 'vmin': 0, 'vmax': 5.0},\n",
    "            'baseline': {'adaptive': True, 'vmin': 0, 'vmax': 1},\n",
    "            'amplitude': {'adaptive': True, 'vmin': 0, 'vmax': 1},\n",
    "            'hrf_delay': {'adaptive': True, 'vmin': 0, 'vmax': 1},\n",
    "            'hrf_dispersion': {'adaptive': True, 'vmin': 0, 'vmax': 1} # Fixed typo\n",
    "        }\n",
    "        \n",
    "        # Special case for pRF eccentricity\n",
    "        if map_type == 'pRF' and cf_property == 'eccentricity':\n",
    "            return {'adaptive': False, 'vmin': 0, 'vmax': 5}\n",
    "        \n",
    "        return property_config.get(cf_property, {'adaptive': True, 'vmin': 0, 'vmax': 1})\n",
    "\n",
    "    # Simplified colormap selection - always uses polar_colors for polar angles\n",
    "    def get_colormap_and_label(self, cf_property):\n",
    "        \"\"\"Selects the appropriate colormap and colorbar label.\"\"\"\n",
    "        if cf_property == 'eccentricity':\n",
    "            grad_cmap = eccen_colors['matplotlib_cmap']\n",
    "            cbar_label = r'Eccentricity $r$ (deg)'\n",
    "        elif cf_property in ('polar', 'polar_angle'):\n",
    "            grad_cmap = polar_colors['matplotlib_cmap'] # Always use polar_colors\n",
    "            cbar_label = r'Polar angle $\\theta$ (rad)'\n",
    "        elif cf_property == 'r2':\n",
    "            grad_cmap = plt.colormaps['viridis']\n",
    "            cbar_label = r'$R^2$'\n",
    "        elif cf_property == 'cf_size':\n",
    "            grad_cmap = plt.colormaps['viridis']\n",
    "            cbar_label = r'CF size $\\sigma$ (mm)'\n",
    "        elif cf_property == 'size':\n",
    "            grad_cmap = plt.colormaps['viridis']\n",
    "            cbar_label = r'pRF size $\\sigma$ (deg)'\n",
    "        elif cf_property in ['baseline', 'amplitude', 'hrf_delay', 'hrf_dispersion']: # Fixed typo\n",
    "            grad_cmap = plt.colormaps['viridis'] # Or specific ones if desired\n",
    "            labels = {\n",
    "                'baseline': 'Baseline (a.u.)',\n",
    "                'amplitude': 'Amplitude (a.u.)',\n",
    "                'hrf_delay': r'HRF delay ($s$)',\n",
    "                'hrf_dispersion': 'HRF dispersion' # Fixed typo\n",
    "            }\n",
    "            cbar_label = labels.get(cf_property, 'Value')\n",
    "        else:\n",
    "            grad_cmap = plt.colormaps['viridis']\n",
    "            cbar_label = 'Value'\n",
    "        return grad_cmap, cbar_label\n",
    "\n",
    "    def apply_coordinate_transformations(self, lh_mesh, rh_mesh, hemi_separation, angle=0):\n",
    "        \"\"\"Applies rotation and separation to mesh coordinates.\"\"\"\n",
    "        lh_coords_plot = lh_mesh.coordinates\n",
    "        rh_coords_plot = rh_mesh.coordinates\n",
    "        lh_faces_plot = lh_mesh.tess.faces\n",
    "        rh_faces_plot = rh_mesh.tess.faces\n",
    "        \n",
    "        # Rotate and shift\n",
    "        lh_coords_medial = rotate_coords(lh_coords_plot, axis='z', angle_degrees=-angle)\n",
    "        rh_coords_medial = rotate_coords(rh_coords_plot, axis='z', angle_degrees=angle * 2)\n",
    "        rh_coords_medial[0, :] += hemi_separation\n",
    "        \n",
    "        # Create shifted meshes\n",
    "        lh_mesh_shifted = Mesh(Tesselation(lh_faces_plot), lh_coords_medial)\n",
    "        rh_mesh_shifted = Mesh(Tesselation(rh_faces_plot), rh_coords_medial)\n",
    "        \n",
    "        return lh_mesh_shifted, rh_mesh_shifted\n",
    "\n",
    "    def prepare_underlay_and_masks(self, lh_curv_map, rh_curv_map, lh_data, rh_data):\n",
    "        \"\"\"Prepares underlay data (curvature) and masks for plotting.\"\"\"\n",
    "        # Prepare underlay (strips) - negative curvature values\n",
    "        lh_strips_plot = lh_curv_map.astype(float)\n",
    "        lh_strips_plot[lh_strips_plot > 0] = np.nan\n",
    "        rh_strips_plot = rh_curv_map.astype(float)\n",
    "        rh_strips_plot[rh_strips_plot > 0] = np.nan\n",
    "        # Prepare underlay (strips) - negative curvature values\n",
    "        # Prepare masks for valid data points\n",
    "        lh_mask_plot = ~np.isnan(lh_data)\n",
    "        rh_mask_plot = ~np.isnan(rh_data)\n",
    "        \n",
    "        return lh_strips_plot, rh_strips_plot, lh_mask_plot, rh_mask_plot\n",
    "\n",
    "    def get_or_create_transformed_meshes(self, lh_mesh, rh_mesh, hemi_separation):\n",
    "        \"\"\"Checks cache for transformed meshes, otherwise creates and caches them.\"\"\"\n",
    "        # For simplicity in this refactor, we recompute the transformation every time,\n",
    "        # but the *loading* of the base meshes (lh_mesh, rh_mesh) is cached.\n",
    "        # The transformation itself (rotate_coords, shift X) is relatively fast.\n",
    "        return self.apply_coordinate_transformations(lh_mesh, rh_mesh, hemi_separation, angle=0)\n",
    "\n",
    "def plot_and_save_brains(lh_map, rh_map, colormap, mesh_lh, mesh_rh, strips_lh, strips_rh, mask_lh, mask_rh, view, vmin=None, vmax=None, cbar_label='Value', cf_property='r2'):\n",
    "    \"\"\"\n",
    "    Plot brain surfaces with given maps and colormap, set the view based on the flag, and save to PNG.\n",
    "    NOTE: polar_colormap parameter removed.\n",
    "    \n",
    "    Parameters:\n",
    "    - lh_map: array-like, map data for left hemisphere\n",
    "    - rh_map: array-like, map data for right hemisphere\n",
    "    - colormap: matplotlib colormap object\n",
    "    - mesh_lh: mesh object for left hemisphere\n",
    "    - mesh_rh: mesh object for right hemisphere\n",
    "    - strips_lh: underlay data for left hemisphere\n",
    "    - strips_rh: underlay data for right hemisphere\n",
    "    - mask_lh: mask for left hemisphere\n",
    "    - mask_rh: mask for right hemisphere\n",
    "    - view: str ('ventral' or 'dorsal') or tuple (azim, elev, dist) to set the camera view\n",
    "    - vmin: float, minimum value for colormap scaling (optional)\n",
    "    - vmax: float, maximum value for colormap scaling (optional)\n",
    "    - cbar_label: str, label for the colorbar (optional, default: 'Value')\n",
    "    - cf_property: str, CF property being plotted (for specialized colorbar insets)\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(view, tuple) and len(view) == 3:\n",
    "        azim, elev, dist = view\n",
    "    elif view == 'ventral':\n",
    "        azim, elev, dist = -172, -8, 180\n",
    "    elif view == 'dorsal':\n",
    "        azim, elev, dist = -6.13, 31.34, 46.26\n",
    "    else:\n",
    "        raise ValueError(\"view must be 'ventral', 'dorsal', or a tuple (azim, elev, dist)\")\n",
    "    \n",
    "    # Create figure\n",
    "    fig = ipv.figure(width=640, height=480)\n",
    "    \n",
    "    # Plot right hemisphere\n",
    "    ny.cortex_plot(mesh_rh, surface='inflated', color=rh_map, cmap=colormap,\n",
    "        underlay=strips_rh, underlay_cmap='gray', underlay_vmin=-5, underlay_vmax=0.0, mask=mask_rh,\n",
    "        vmin=vmin, vmax=vmax,\n",
    "        figure=fig)\n",
    "    \n",
    "    # Plot left hemisphere\n",
    "    ny.cortex_plot(mesh_lh, surface='inflated', color=lh_map, cmap=colormap,\n",
    "        underlay=strips_lh, underlay_cmap='gray', underlay_vmin=-5, underlay_vmax=0.0, mask=mask_lh,\n",
    "        vmin=vmin, vmax=vmax,\n",
    "        figure=fig)\n",
    "    \n",
    "    # Compute the center of the plot (mean of all mesh coordinates)\n",
    "    all_coords = np.concatenate([mesh_lh.coordinates, mesh_rh.coordinates], axis=1)\n",
    "    center = np.mean(all_coords, axis=1)\n",
    "    fig.camera.center = center\n",
    "    \n",
    "    # Custom function to set view relative to center\n",
    "    def set_view(fig, azimuth, elevation, distance):\n",
    "        center = fig.camera.center\n",
    "        elev_rad = np.radians(elevation)\n",
    "        az_rad = np.radians(azimuth)\n",
    "        unit = np.array([\n",
    "            np.cos(elev_rad) * np.cos(az_rad),\n",
    "            np.cos(elev_rad) * np.sin(az_rad),\n",
    "            np.sin(elev_rad)\n",
    "        ])\n",
    "        fig.camera.position = tuple(center + distance * unit)\n",
    "    \n",
    "    # Adjust the final view\n",
    "    set_view(fig, azim, elev, dist)\n",
    "    ipv.show()\n",
    "    \n",
    "    # Add colorbar using inset - property-specific\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import cm\n",
    "    from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "    from matplotlib.patches import Wedge\n",
    "    \n",
    "    # Normalize values\n",
    "    vmin_val = vmin if vmin is not None else np.nanmin([np.nanmin(lh_map), np.nanmin(rh_map)])\n",
    "    vmax_val = vmax if vmax is not None else np.nanmax([np.nanmax(lh_map), np.nanmax(rh_map)])\n",
    "    \n",
    "    if cf_property == 'eccentricity':\n",
    "        # Create radial concentric rings colorbar for eccentricity\n",
    "        fig_cb, ax_main = plt.subplots(figsize=(3, 3))\n",
    "        ax_main.set_aspect('equal')\n",
    "        ax_main.set_xlim(-1.5, 1.5)\n",
    "        ax_main.set_ylim(-1.5, 1.5)\n",
    "        ax_main.set_axis_off()\n",
    "        ax_main.text(0.5, -0.05, r'$\\mathit{r}\\ (\\mathrm{deg})$', ha='center', va='top', \n",
    "                    fontsize=14, transform=ax_main.transAxes)\n",
    "        \n",
    "        num_ecc_colors = len(eccen_colors[\"hex\"])\n",
    "        for i, color in enumerate(eccen_colors[\"hex\"]):\n",
    "            inner_r = i / num_ecc_colors\n",
    "            outer_r = (i + 1) / num_ecc_colors\n",
    "            ring = Wedge((0, 0), outer_r, 0, 360, width=outer_r - inner_r, color=color)\n",
    "            ax_main.add_patch(ring)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    elif cf_property in ('polar', 'polar_angle'):\n",
    "        # Create polar pie chart colorbar for polar angle - Always uses polar_colors now\n",
    "        fig_cb, ax_main = plt.subplots(figsize=(3, 3))\n",
    "        ax_main.set_aspect('equal')\n",
    "        ax_main.set_axis_off()\n",
    "        # Use custom polar colormap (previously 'polar' option)\n",
    "        ax_main.pie([1] * len(polar_colors[\"hex\"]), colors=polar_colors[\"hex\"], \n",
    "                   startangle=180, counterclock=False)\n",
    "    \n",
    "        ax_main.text(0.5, -0.05, r'$\\theta\\ (\\mathrm{rad})$', ha='center', va='top', \n",
    "                    fontsize=14, transform=ax_main.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Create standard horizontal colorbar for other properties\n",
    "        fig_cb, ax_main = plt.subplots(figsize=(8, 2))\n",
    "        ax_main.set_axis_off()\n",
    "        \n",
    "        # Create inset for horizontal colorbar\n",
    "        cbar_inset = inset_axes(ax_main, width=\"70%\", height=\"30%\", loc=\"center\", borderpad=0)\n",
    "        \n",
    "        norm = plt.Normalize(vmin=vmin_val, vmax=vmax_val)\n",
    "        \n",
    "        # Create colorbar in inset\n",
    "        cb = plt.colorbar(cm.ScalarMappable(norm=norm, cmap=colormap),\n",
    "                          cax=cbar_inset, orientation='horizontal')\n",
    "        cb.set_label(cbar_label, fontsize=14)\n",
    "        cb.ax.tick_params(labelsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Create widgets for real-time updates\n",
    "    # azimuth_widget = FloatText(description='Azimuth:', step=0.1, disabled=True)\n",
    "    # elevation_widget = FloatText(description='Elevation:', step=0.1, disabled=True)\n",
    "    # distance_widget = FloatText(description='Distance:', step=0.1, disabled=True)\n",
    "\n",
    "    # set_view_widget = Textarea(\n",
    "    #     description='set_view call:',\n",
    "    #     value='set_view(fig, 0.00, 0.00, 0.00)',\n",
    "    #     disabled=True,\n",
    "    #     layout={'width': '400px', 'height': '50px'}\n",
    "    # )\n",
    "\n",
    "    # def update_widgets(change):\n",
    "    #     pos = fig.camera.position\n",
    "    #     center = fig.camera.center\n",
    "    #     v = np.array(pos) - np.array(center)\n",
    "    #     dist = np.linalg.norm(v)\n",
    "    #     if dist > 0:\n",
    "    #         elevation = np.degrees(np.arcsin(v[2] / dist))\n",
    "    #         azimuth = np.degrees(np.arctan2(v[1], v[0]))\n",
    "    #     else:\n",
    "    #         azimuth = 0\n",
    "    #         elevation = 0\n",
    "    #     distance = dist\n",
    "    #     azimuth_widget.value = azimuth\n",
    "    #     elevation_widget.value = elevation\n",
    "    #     distance_widget.value = distance\n",
    "    #     set_view_widget.value = f\"set_view(fig, {azimuth:.2f}, {elevation:.2f}, {distance:.2f})\"\n",
    "    #     print(set_view_widget.value)\n",
    "\n",
    "    # fig.camera.observe(update_widgets, names=['position'])\n",
    "    # update_widgets(None)\n",
    "\n",
    "    # # Display the widgets\n",
    "    # display(VBox([HBox([azimuth_widget, elevation_widget, distance_widget]), set_view_widget]))\n",
    "    # # Display the widgets\n",
    "\n",
    "# Main update function (reconstructed with state management)\n",
    "def update_plot_refactored_with_state(state_manager, dataset_key, subject_id, task, source_hemi, map_type,\n",
    "                           cf_property, r2_threshold, use_adaptive_range, vmin, vmax,\n",
    "                           hemi_separation): # Removed polar_colormap parameter\n",
    "    \"\"\"Main update function using refactored components and state manager.\"\"\"\n",
    "    \n",
    "    # --- 1. Determine Paths and Configuration ---\n",
    "    base_data_path, cf_models_dir, fs_subjects_dir = state_manager.get_paths_and_configs(dataset_key)\n",
    "    current_sub_id_formatted = f\"{int(subject_id):02d}\"\n",
    "    fs_subject_name = DATASET_CONFIGS[dataset_key]['fs_subject_format'].format(subject_id=current_sub_id_formatted)\n",
    "    fs_subject_path = fs_subjects_dir / fs_subject_name\n",
    "    mesh_cache_key = f\"{dataset_key}_{current_sub_id_formatted}\"\n",
    "    \n",
    "    # --- 2. Load Data (CF or pRF) ---\n",
    "    if map_type == 'CF':\n",
    "        # Load CF results\n",
    "        available_tasks = [info['task'] for info in cf_models_info if info['subject'] == subject_id and info['source_hemi'] == source_hemi]\n",
    "        available_tasks = sorted(list(set(available_tasks)))\n",
    "        if task not in available_tasks:\n",
    "             print(f\"⚠️ Task '{task}' not available for sub-{subject_id}, source-{source_hemi}. Available: {available_tasks}\")\n",
    "             return # Exit early if invalid task\n",
    "        cf_results_lh, cf_results_rh = state_manager.load_cf_results(subject_id, task, source_hemi, dataset_key)\n",
    "        if cf_results_lh is None or cf_results_rh is None:\n",
    "            return # Error already printed in load function\n",
    "\n",
    "        # Map property to CF result key\n",
    "        prop_map = {\n",
    "            'eccentricity': 'inherited_eccen', 'polar': 'inherited_polar',\n",
    "            'cf_size': 'cf_size', 'r2': 'r2'\n",
    "        }\n",
    "        if cf_property not in prop_map:\n",
    "            print(f\"Unknown CF property: {cf_property}\")\n",
    "            return\n",
    "\n",
    "        # Extract data and R2\n",
    "        lh_data_raw = cf_results_lh[prop_map[cf_property]]\n",
    "        rh_data_raw = cf_results_rh[prop_map[cf_property]]\n",
    "        lh_r2_raw = cf_results_lh.get('r2', np.full_like(lh_data_raw, np.nan))\n",
    "        rh_r2_raw = cf_results_rh.get('r2', np.full_like(rh_data_raw, np.nan))\n",
    "\n",
    "    elif map_type == 'pRF':\n",
    "        # Load pRF results\n",
    "        prf_results_lh, prf_results_rh = state_manager.load_prf_results(subject_id, dataset_key)\n",
    "        if prf_results_lh is None or prf_results_rh is None:\n",
    "            return # Error already printed in load function\n",
    "\n",
    "        # Prepare full data arrays from dictionary\n",
    "        # This requires the mesh shape, so we load the mesh first if needed for pRF\n",
    "        mesh_data = state_manager.load_meshes_and_curv(fs_subject_path, mesh_cache_key)\n",
    "        if mesh_data is None:\n",
    "             return # Error already printed in load function\n",
    "        lh_mesh_shape = mesh_data['lh_mesh'].coordinates.shape\n",
    "        rh_mesh_shape = mesh_data['rh_mesh'].coordinates.shape\n",
    "        \n",
    "        lh_data_raw, rh_data_raw, lh_r2_raw, rh_r2_raw = state_manager.prepare_prf_data_arrays(\n",
    "            prf_results_lh, prf_results_rh, lh_mesh_shape, rh_mesh_shape, cf_property\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Unknown map_type: {map_type}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Apply R2 Mask (Could potentially cache this step too) ---\n",
    "    lh_data_masked, rh_data_masked, lh_mask, rh_mask = state_manager.apply_r2_mask(\n",
    "        lh_data_raw, rh_data_raw, lh_r2_raw, rh_r2_raw, r2_threshold\n",
    "    )\n",
    "\n",
    "    # --- 4. Load Meshes and Curvature (Cached) ---\n",
    "    mesh_data = state_manager.load_meshes_and_curv(fs_subject_path, mesh_cache_key)\n",
    "    if mesh_data is None:\n",
    "         return # Error already printed in load function\n",
    "    lh_mesh, rh_mesh = mesh_data['lh_mesh'], mesh_data['rh_mesh']\n",
    "    lh_curv_map, rh_curv_map = mesh_data['lh_curv_map'], mesh_data['rh_curv_map']\n",
    "\n",
    "    # --- 5. Determine Value Range (Adaptive or Fixed) ---\n",
    "    if use_adaptive_range:\n",
    "        valid_data = np.concatenate([lh_data_masked[~np.isnan(lh_data_masked)], rh_data_masked[~np.isnan(rh_data_masked)]])\n",
    "        if valid_data.size > 0:\n",
    "            vmin_calc = np.percentile(valid_data, 2)\n",
    "            vmax_calc = np.percentile(valid_data, 98)\n",
    "        else:\n",
    "            # Fallback if no valid data passes R2 threshold\n",
    "            fallback_cfg = state_manager.get_property_specific_config(cf_property, map_type)\n",
    "            vmin_calc = fallback_cfg['vmin']\n",
    "            vmax_calc = fallback_cfg['vmax']\n",
    "    else:\n",
    "        vmin_calc = vmin\n",
    "        vmax_calc = vmax\n",
    "\n",
    "    # --- 6. Prepare Underlay and Masks for Plotting ---\n",
    "    lh_strips_plot, rh_strips_plot, lh_mask_plot, rh_mask_plot = state_manager.prepare_underlay_and_masks(\n",
    "        lh_curv_map, rh_curv_map, lh_data_masked, rh_data_masked\n",
    "    )\n",
    "\n",
    "    # --- 7. Apply Coordinate Transformations (Cached or Recomputed) ---\n",
    "    # As discussed, caching the transformed mesh based on separation might be complex.\n",
    "    # We recompute the transformation here, relying on the cached base meshes (lh_mesh, rh_mesh).\n",
    "    lh_mesh_final, rh_mesh_final = state_manager.get_or_create_transformed_meshes(\n",
    "        lh_mesh, rh_mesh, hemi_separation\n",
    "    )\n",
    "\n",
    "    # --- 8. Select Colormap and Label (Simplified) ---\n",
    "    grad_cmap, cbar_label = state_manager.get_colormap_and_label(cf_property)\n",
    "    \n",
    "    # --- 9. Finalize Plot ---\n",
    "    try:\n",
    "        plot_and_save_brains(\n",
    "            lh_data_masked, rh_data_masked, grad_cmap,\n",
    "            lh_mesh_final, rh_mesh_final,\n",
    "            lh_strips_plot, rh_strips_plot,\n",
    "            lh_mask_plot, rh_mask_plot,\n",
    "            (-122, -27, 80), vmin=vmin_calc, vmax=vmax_calc,\n",
    "            cbar_label=cbar_label, cf_property=cf_property\n",
    "        )\n",
    "        # Update current plot state\n",
    "        state_manager.current_plot_data = {\n",
    "            'lh_data': lh_data_masked, 'rh_data': rh_data_masked,\n",
    "            'lh_mesh': lh_mesh_final, 'rh_mesh': rh_mesh_final,\n",
    "            'colormap': grad_cmap, 'vmin': vmin_calc, 'vmax': vmax_calc,\n",
    "            'cbar_label': cbar_label, 'subject_id': subject_id, 'task': task,\n",
    "            'source_hemi': source_hemi, 'cf_property': cf_property,\n",
    "            'r2_threshold': r2_threshold, 'map_type': map_type\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error plotting brains: {e}\")\n",
    "        return\n",
    "\n",
    "# Widget setup and main loop (cell 3 equivalent)\n",
    "# Dataset configurations (Updated order and removed polar_colormap widget)\n",
    "DATASET_CONFIGS = {\n",
    "    'iCRTX7T': { # Moved to top\n",
    "        'name': 'iCORTEX (7T)',\n",
    "        'task_pattern': r'cf_results_sub-(\\d+)_(ses-\\d+)_([\\w-]+)_(lh|rh)-source_ecc([\\d.]+)-([\\d.]+)\\.npz',\n",
    "        'fs_subject_format': 'sub-{subject_id}_ses-01_iso'\n",
    "    },\n",
    "    'LPP7T': {\n",
    "        'name': 'Le Petit Prince (7T)',\n",
    "        'task_pattern': r'cf_results_sub-(\\d+)_(ses-\\d+)_(\\w+)_(lh|rh)-source_ecc([\\d.]+)-([\\d.]+)\\.npz',\n",
    "        'fs_subject_format': 'sub-{subject_id}_ses-01_iso'\n",
    "    },\n",
    "    'CB3T': {\n",
    "        'name': 'Congenital Blindness (3T)',\n",
    "        'task_pattern': r'cf_results_sub-(\\d+)_(ses-\\d+)_(\\w+)_(lh|rh)-source_ecc([\\d.]+)-([\\d.]+)\\.npz',\n",
    "        'fs_subject_format': 'sub-{subject_id}_ses-01_iso'\n",
    "    },\n",
    "}\n",
    "# Flag for pRF availability (already present in iCRTX7T key)\n",
    "DATASET_CONFIGS['iCRTX7T']['has_prf'] = True\n",
    "DATASET_CONFIGS['LPP7T']['has_prf'] = False\n",
    "DATASET_CONFIGS['CB3T']['has_prf'] = False\n",
    "DATASET_CONFIGS['LPP7T']['has_prf'] = False\n",
    "\n",
    "# Initialize with iCRTX7T dataset (Updated default)\n",
    "current_dataset_key = 'iCRTX7T'\n",
    "notebook_dir = Path().resolve() # Access notebook dir\n",
    "base_data_path = notebook_dir / 'data' / current_dataset_key\n",
    "cf_models_dir = base_data_path / 'derivatives' / 'cf-models'\n",
    "fs_subjects_dir = base_data_path / 'fs_subjects'\n",
    "\n",
    "if not base_data_path.exists():\n",
    "    print(f\"⚠️ No data found for {DATASET_CONFIGS[current_dataset_key]['name']}\")\n",
    "    print(f\"Expected path: {base_data_path}\")\n",
    "    print(\"\\nPlease ensure data is organized as:\")\n",
    "    print(\"  data/\")\n",
    "    print(\"    iCRTX7T/\") # Updated example\n",
    "    print(\"      derivatives/cf-models/\")\n",
    "    print(\"      fs_subjects/\")\n",
    "    print(\"    LPP7T/\")\n",
    "    print(\"      derivatives/cf-models/\")\n",
    "    print(\"      fs_subjects/\")\n",
    "    print(\"    CB3T/\")\n",
    "    print(\"      derivatives/cf-models/\")\n",
    "    print(\"      fs_subjects/\")\n",
    "\n",
    "# Scan available CF model files\n",
    "def scan_cf_models(cf_models_dir, task_pattern):\n",
    "    \"\"\"Scan CF models directory and extract available subjects, tasks, and sources.\"\"\"\n",
    "    cf_models_dir = Path(cf_models_dir)\n",
    "    pattern = str(cf_models_dir / '*.npz')\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    available_models = []\n",
    "    subjects = set()\n",
    "    tasks = set()\n",
    "    \n",
    "    for f in files:\n",
    "        basename = Path(f).name\n",
    "        # Use dataset-specific pattern\n",
    "        match = re.match(task_pattern, basename)\n",
    "        if match:\n",
    "            subject_id, session_id, task, source_hemi, min_ecc, max_ecc = match.groups()\n",
    "            subjects.add(subject_id)\n",
    "            tasks.add(task)\n",
    "            available_models.append({\n",
    "                'file': f,\n",
    "                'subject': subject_id,\n",
    "                'session': session_id,\n",
    "                'task': task,\n",
    "                'source_hemi': source_hemi,\n",
    "                'min_ecc': float(min_ecc),\n",
    "                'max_ecc': float(max_ecc)\n",
    "            })\n",
    "    \n",
    "    return sorted(list(subjects)), sorted(list(tasks)), available_models\n",
    "\n",
    "# Scan models with current dataset pattern\n",
    "available_subjects, available_tasks, cf_models_info = scan_cf_models(\n",
    "    cf_models_dir, \n",
    "    DATASET_CONFIGS[current_dataset_key]['task_pattern']\n",
    ")\n",
    "\n",
    "# Instantiate state manager\n",
    "state_manager = CFVisualizationState()\n",
    "\n",
    "# Flag to prevent recursive updates\n",
    "updating = False\n",
    "\n",
    "# Widget functions (callbacks)\n",
    "def update_dataset_options(change):\n",
    "    \"\"\"Update subject and task options when dataset changes.\"\"\"\n",
    "    global available_subjects, available_tasks, cf_models_info, base_data_path, cf_models_dir, fs_subjects_dir, current_dataset_key\n",
    "    \n",
    "    new_dataset_key = change['new']\n",
    "    current_dataset_key = new_dataset_key\n",
    "    \n",
    "    # Update base path to new dataset folder\n",
    "    base_data_path = notebook_dir / 'data' / new_dataset_key\n",
    "    \n",
    "    if not base_data_path.exists():\n",
    "        print(f\"⚠️ No data found for {DATASET_CONFIGS[new_dataset_key]['name']}\")\n",
    "        print(f\"Expected path: {base_data_path}\")\n",
    "        return\n",
    "    \n",
    "    # Update paths\n",
    "    cf_models_dir = base_data_path / 'derivatives' / 'cf-models'\n",
    "    fs_subjects_dir = base_data_path / 'fs_subjects'\n",
    "    \n",
    "    # Rescan with new pattern\n",
    "    task_pattern = DATASET_CONFIGS[new_dataset_key]['task_pattern']\n",
    "    available_subjects_new, available_tasks_new, cf_models_info_new = scan_cf_models(cf_models_dir, task_pattern)\n",
    "    \n",
    "    # Update global variables and widget options\n",
    "    available_subjects[:] = available_subjects_new\n",
    "    available_tasks[:] = available_tasks_new\n",
    "    cf_models_info[:] = cf_models_new\n",
    "    \n",
    "    subject_widget.options = available_subjects\n",
    "    subject_widget.value = available_subjects[0] if available_subjects else '01'\n",
    "    task_widget.options = available_tasks\n",
    "    task_widget.value = available_tasks[0] if available_tasks else 'LPP1'\n",
    "    \n",
    "    # Update map type options based on pRF availability\n",
    "    #has_prf = DATASET_CONFIGS[new_dataset_key].get('has_prf', False)\n",
    "    has_prf = DATASET_CONFIGS[new_dataset_key].get('has_prf', True)\n",
    "    if has_prf:\n",
    "        map_type_widget.options = ['CF', 'pRF']\n",
    "        map_type_widget.value = 'pRF'  # Default to pRF\n",
    "    else:\n",
    "        map_type_widget.options = ['CF']\n",
    "        map_type_widget.value = 'CF'\n",
    "    if DEBUG:\n",
    "        print(f\"✓ Switched to {DATASET_CONFIGS[new_dataset_key]['name']}\")\n",
    "        print(f\"  Found {len(available_subjects)} subjects, {len(available_tasks)} tasks\")\n",
    "        if has_prf:\n",
    "            print(\"  pRF data available\")\n",
    "\n",
    "def update_map_type_options(change):\n",
    "    \"\"\"Update property options and widget visibility when map type changes.\"\"\"\n",
    "    global updating\n",
    "    if updating:\n",
    "        return\n",
    "    updating = True\n",
    "    \n",
    "    try:\n",
    "        map_type = change['new']\n",
    "        if map_type == 'CF':\n",
    "            cf_property_widget.options = ['eccentricity', 'polar', 'cf_size', 'r2']\n",
    "            cf_property_widget.value = 'eccentricity'\n",
    "            task_widget.layout.display = 'flex'  # Show task\n",
    "            source_hemi_widget.layout.display = 'flex' # Show source\n",
    "            cf_property_widget.description = 'CF parameter:'\n",
    "        elif map_type == 'pRF':\n",
    "            cf_property_widget.options = ['eccentricity', 'polar_angle', 'size', 'baseline', 'amplitude', 'hrf_delay', 'hrf_dispersion', 'r2']\n",
    "            cf_property_widget.value = 'eccentricity' # Or whatever makes sense as default for pRF\n",
    "            task_widget.layout.display = 'none'  # Hide task\n",
    "            source_hemi_widget.layout.display = 'none' # Hide source\n",
    "            cf_property_widget.description = 'pRF parameter:'\n",
    "        # Update defaults for the new property\n",
    "        update_widget_defaults(cf_property_widget.value) # Call this to set vmin/vmax/range defaults\n",
    "    finally:\n",
    "        updating = False\n",
    "\n",
    "def update_widget_defaults(cf_property):\n",
    "    \"\"\"Update adaptive range and min/max defaults based on CF property and map type.\n",
    "    Note: This function is always called from within a protected context (where updating is already True),\n",
    "    so it doesn't need to check or modify the updating flag.\"\"\"\n",
    "    map_type = map_type_widget.value  # Get current map type\n",
    "    \n",
    "    config = state_manager.get_property_specific_config(cf_property, map_type)\n",
    "    adaptive_range_widget.value = config['adaptive']\n",
    "    vmin_widget.value = config['vmin']\n",
    "    vmax_widget.value = config['vmax']\n",
    "\n",
    "# Removed update_widget_visibility as polar_colormap_widget is gone\n",
    "\n",
    "# Widget creation\n",
    "# Updated order of dataset options to prioritize iCRTX7T\n",
    "dataset_widget = Dropdown(\n",
    "    options=[(config['name'], key) for key, config in DATASET_CONFIGS.items()],\n",
    "    value='iCRTX7T', # Changed default\n",
    "    description='Dataset:'\n",
    ")\n",
    "subject_widget = Dropdown(options=available_subjects, \n",
    "                         value=available_subjects[0] if available_subjects else '01', \n",
    "                         description='Subject:')\n",
    "task_widget = Dropdown(options=available_tasks, \n",
    "                      value=available_tasks[0] if available_tasks else 'LPP1', \n",
    "                      description='Task:')\n",
    "source_hemi_widget = Dropdown(options=['lh', 'rh'], value='lh', description='Source:')\n",
    "\n",
    "map_type_widget = Dropdown(options=['CF', 'pRF'], value='pRF', description='Map type:') \n",
    "\n",
    "cf_property_widget = Dropdown(options=['eccentricity', 'polar', 'cf_size', 'r2'], \n",
    "                              value='eccentricity', description='Property:')\n",
    "r2_threshold_widget = FloatSlider(value=0.1, min=0.0, max=1.0, step=0.01, description='R² threshold:')\n",
    "\n",
    "# Adaptive range widget - default depends on CF property\n",
    "adaptive_range_widget = Dropdown(options=[True, False], value=False, \n",
    "                                description='Range:')\n",
    "r2_threshold_widget = FloatSlider(value=0.1, min=0.0, max=1.0, step=0.01, description='R² threshold:')\n",
    "# Min/max widgets - defaults depend on CF property - initialized for eccentricity\n",
    "vmin_widget = FloatSlider(value=0.5, min=-10, max=10, step=0.01, description='min:')\n",
    "vmax_widget = FloatSlider(value=6.5, min=-10, max=10, step=0.01, description='max:')\n",
    "\n",
    "# REMOVED: polar_colormap_widget\n",
    "# Min/max widgets - defaults depend on CF property - initialized for eccentricity\n",
    "# Hemisphere separation slider\n",
    "hemi_separation_widget = FloatSlider(\n",
    "    value=80, \n",
    "    min=0, \n",
    "    max=200, \n",
    "    step=1, \n",
    "    description='Hemi gap:',\n",
    "    tooltip='Distance between left and right hemispheres'\n",
    ")\n",
    "# Trigger initial layout update for pRF map type\n",
    "update_map_type_options({'new': map_type_widget.value})\n",
    "\n",
    "# Link widget callbacks\n",
    "dataset_widget.observe(update_dataset_options, names='value')\n",
    "map_type_widget.observe(update_map_type_options, names='value')\n",
    "\n",
    "# Protected callback for cf_property changes\n",
    "def on_cf_property_change(change):\n",
    "    global updating\n",
    "    if updating:\n",
    "        return\n",
    "    \n",
    "    updating = True  # Block all other callbacks during entire sequence\n",
    "    \n",
    "    # Temporarily unobserve the widgets we're about to change\n",
    "    adaptive_range_widget.unobserve(update_visualization, names='value')\n",
    "    vmin_widget.unobserve(update_visualization, names='value')\n",
    "    vmax_widget.unobserve(update_visualization, names='value')\n",
    "    \n",
    "    try:\n",
    "        # Update the widgets\n",
    "        update_widget_defaults(change['new'])\n",
    "    finally:\n",
    "        # Re-attach observers\n",
    "        adaptive_range_widget.observe(update_visualization, names='value')\n",
    "        vmin_widget.observe(update_visualization, names='value')\n",
    "        vmax_widget.observe(update_visualization, names='value')\n",
    "    \n",
    "    # Now trigger single visualization update while updating is still True\n",
    "    # This prevents any other callbacks from interfering\n",
    "    update_visualization()\n",
    "    \n",
    "    # Only reset flag after everything is done\n",
    "    updating = False\n",
    "\n",
    "cf_property_widget.observe(on_cf_property_change, names='value')\n",
    "# Removed observe for update_widget_visibility\n",
    "\n",
    "# =============================\n",
    "# CLEAN RENDERING WITH MANUAL CALLBACKS\n",
    "# =============================\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython.display import HTML\n",
    "# CLEAN RENDERING WITH MANUAL CALLBACKS\n",
    "# Output areas\n",
    "loading_output = Output()\n",
    "plot_output = Output()\n",
    "\n",
    "def show_loading():\n",
    "    with loading_output:\n",
    "        loading_output.clear_output(wait=True)\n",
    "        display(HTML(\"\"\"\n",
    "            <div style=\"text-align: center; margin: 20px;\">\n",
    "                <div style=\"width: 40px; height: 40px; border: 4px solid rgba(0,0,0,0.1);\n",
    "                            border-left-color: #4A90E2; border-radius: 50%;\n",
    "                            animation: spin 1s linear infinite;\"></div>\n",
    "                <p style=\"margin-top: 10px; color: #555;\">Loading...</p>\n",
    "            </div>\n",
    "            <style>@keyframes spin { to { transform: rotate(360deg); } }</style>\n",
    "        \"\"\"))\n",
    "\n",
    "def hide_loading():\n",
    "    with loading_output:\n",
    "        loading_output.clear_output()\n",
    "\n",
    "# Unified update function\n",
    "def update_visualization(*args, from_protected_callback=False):\n",
    "    import time\n",
    "    t_start = time.time()\n",
    "    \n",
    "    global updating\n",
    "    # Skip updating check if called from protected callback\n",
    "    if not from_protected_callback and updating:\n",
    "        return\n",
    "    if DEBUG:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🔄 UPDATE VISUALIZATION TRIGGERED\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # Get current values\n",
    "    dataset_key = dataset_widget.value\n",
    "    subject_id = subject_widget.value\n",
    "    task = task_widget.value\n",
    "    source_hemi = source_hemi_widget.value\n",
    "    map_type = map_type_widget.value\n",
    "    cf_property = cf_property_widget.value\n",
    "    r2_threshold = r2_threshold_widget.value\n",
    "    use_adaptive_range = adaptive_range_widget.value\n",
    "    vmin = vmin_widget.value\n",
    "    vmax = vmax_widget.value\n",
    "    hemi_separation = hemi_separation_widget.value\n",
    "\n",
    "    # Clear old plot\n",
    "    with plot_output:\n",
    "        plot_output.clear_output()\n",
    "    show_loading()\n",
    "\n",
    "    try:\n",
    "        from time import sleep\n",
    "        sleep(0.05)\n",
    "        t_plot_start = time.time()\n",
    "        with plot_output:\n",
    "            update_plot_refactored_with_state(\n",
    "                state_manager=state_manager,\n",
    "                dataset_key=dataset_key,\n",
    "                subject_id=subject_id,\n",
    "                task=task,\n",
    "                source_hemi=source_hemi,\n",
    "                map_type=map_type,\n",
    "                cf_property=cf_property,\n",
    "                r2_threshold=r2_threshold,\n",
    "                use_adaptive_range=use_adaptive_range,\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "                hemi_separation=hemi_separation\n",
    "            )\n",
    "        if DEBUG:\n",
    "            print(f\"\\n⏱️ Plot function took: {time.time()-t_plot_start:.2f}s\")\n",
    "    except Exception as e:\n",
    "        with plot_output:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "    finally:\n",
    "        hide_loading()\n",
    "        if DEBUG:\n",
    "            print(f\"✅ Total update time: {time.time()-t_start:.2f}s\")\n",
    "            print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Attach callback to all relevant widgets\n",
    "# NOTE: cf_property_widget is NOT in this list because it has its own\n",
    "# protected callback (on_cf_property_change) that handles widget updates\n",
    "for widget in [\n",
    "    dataset_widget, subject_widget, task_widget, source_hemi_widget,\n",
    "    map_type_widget, r2_threshold_widget,\n",
    "    adaptive_range_widget, vmin_widget, vmax_widget, hemi_separation_widget\n",
    "]:\n",
    "    widget.observe(update_visualization, names='value')\n",
    "    dataset_widget, subject_widget, task_widget, source_hemi_widget,\n",
    "# Initial render\n",
    "#update_visualization()\n",
    "update_visualization(from_protected_callback=True)\n",
    "\n",
    "# Final layout\n",
    "display(VBox([\n",
    "    HBox([\n",
    "        VBox([dataset_widget, subject_widget, map_type_widget, task_widget,\n",
    "              source_hemi_widget, cf_property_widget, adaptive_range_widget]),\n",
    "        VBox([hemi_separation_widget, r2_threshold_widget, vmin_widget, vmax_widget])\n",
    "    ]),\n",
    "    loading_output,\n",
    "\n",
    "    plot_output\n",
    "]))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
