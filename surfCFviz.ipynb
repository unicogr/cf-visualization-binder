{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70a5156",
   "metadata": {},
   "source": [
    "\n",
    "# Whole-brain Connective Field maps\n",
    "\n",
    "Here you can select a CF parameter (e.g., eccentricity or polar angle) and plot it interactively for models referred to left or right V1. Target areas for V1 connectivity include the whole brain. Accordingly, CFs in V1 can project to the contralateral hemisphere, potentially revealing retinotopically organized connectivity in contralateral V1, as well as elsewhere in the cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3086f3-32fe-4b3e-9ba4-96b09ce43de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe imports with error handling\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core imports\n",
    "try:\n",
    "    import os\n",
    "    import glob\n",
    "    import yaml\n",
    "    from pathlib import Path\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Core imports failed: {e}')\n",
    "    raise\n",
    "\n",
    "# Neuroimaging imports\n",
    "try:\n",
    "    import neuropythy as ny\n",
    "    from neuropythy.geometry import Mesh, Tesselation\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Neuropythy failed: {e}')\n",
    "    raise\n",
    "\n",
    "# Visualization imports\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import ipyvolume as ipv\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.patches import Patch, Wedge\n",
    "    import matplotlib.colors as mcolors\n",
    "    from matplotlib import cm\n",
    "    from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Visualization imports failed: {e}')\n",
    "    raise\n",
    "\n",
    "# Additional scientific imports\n",
    "try:\n",
    "    from nilearn.surface import vol_to_surf\n",
    "    from nilearn import surface, plotting, signal\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    from scipy.spatial import cKDTree\n",
    "    from scipy.stats import pearsonr\n",
    "except Exception as e:\n",
    "    pass  # Optional imports, silent fail\n",
    "\n",
    "# Widget imports\n",
    "try:\n",
    "    from ipywidgets import FloatText, HBox, VBox, Textarea, Output, Dropdown, FloatSlider, interactive_output\n",
    "    from traitlets import link\n",
    "    from IPython.display import display\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Widget imports failed: {e}')\n",
    "    raise\n",
    "\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cf5866-ffe1-4cd5-b4a8-b9e857ece56a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Inline color palettes (no external cfmap dependency)\n",
    "def get_eccentricity_palette():\n",
    "    \"\"\"\n",
    "    Returns a color palette with 10 colors transitioning from red to orange to yellow to green to turquoise to cyan to blue.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing different formats of the color palette\n",
    "    \"\"\"\n",
    "    from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # Original RGB values (0-255)\n",
    "    rgb_values = [\n",
    "        [255, 40, 0],    # Red\n",
    "        [255, 130, 0],   # Orange-red\n",
    "        [255, 210, 0],   # Orange-yellow\n",
    "        [255, 255, 0],   # Yellow\n",
    "        [115, 255, 0],   # Yellow-green\n",
    "        [31, 255, 0],    # Green\n",
    "        [0, 255, 207],   # Turquoise\n",
    "        [0, 231, 255],   # Cyan\n",
    "        [20, 140, 255],  # Light blue\n",
    "        [40, 60, 255]    # Blue\n",
    "    ]\n",
    "    \n",
    "    # Normalize to 0-1 range for matplotlib\n",
    "    norm_values = [[r/255, g/255, b/255] for r, g, b in rgb_values]\n",
    "    \n",
    "    # Create hex values\n",
    "    hex_values = [mcolors.rgb2hex(rgb) for rgb in norm_values]\n",
    "    \n",
    "    # Create named colors\n",
    "    named_colors = {f\"color{i+1}\": hex_values[i] for i in range(len(hex_values))}\n",
    "    \n",
    "    return {\n",
    "        \"rgb_0_255\": rgb_values,\n",
    "        \"rgb_0_1\": norm_values,\n",
    "        \"hex\": hex_values,\n",
    "        \"named\": named_colors,\n",
    "        \"matplotlib_cmap\": LinearSegmentedColormap.from_list(\"eccen_cmap\", norm_values)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_polar_palette():\n",
    "    \"\"\"\n",
    "    Returns a color palette with 20 colors transitioning from green to red to green to blue to green.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing different formats of the color palette\n",
    "    \"\"\"\n",
    "    from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # Original RGB values (0-255)\n",
    "    rgb_values = [\n",
    "        [106, 189, 69],   # Color1\n",
    "        [203, 219, 42],   # Color2\n",
    "        [254, 205, 8],    # Color3\n",
    "        [242, 104, 34],   # Color4\n",
    "        [237, 32, 36],    # Color5\n",
    "        [237, 32, 36],    # Color6\n",
    "        [242, 104, 34],   # Color7\n",
    "        [254, 205, 8],    # Color8\n",
    "        [203, 219, 42],   # Color9\n",
    "        [106, 189, 69],   # Color10\n",
    "        [106, 189, 69],   # Color11\n",
    "        [110, 205, 221],  # Color12\n",
    "        [50, 178, 219],   # Color13\n",
    "        [62, 105, 179],   # Color14\n",
    "        [57, 84, 165],    # Color15\n",
    "        [57, 84, 165],    # Color16\n",
    "        [62, 105, 179],   # Color17\n",
    "        [50, 178, 219],   # Color18\n",
    "        [110, 205, 221],  # Color19\n",
    "        [106, 189, 69]    # Color20\n",
    "    ]\n",
    "    \n",
    "    # Normalize to 0-1 range for matplotlib\n",
    "    norm_values = [[r/255, g/255, b/255] for r, g, b in rgb_values]\n",
    "    \n",
    "    # Create hex values\n",
    "    hex_values = [mcolors.rgb2hex(rgb) for rgb in norm_values]\n",
    "    \n",
    "    # Create named colors\n",
    "    named_colors = {f\"color{i+1}\": hex_values[i] for i in range(len(hex_values))}\n",
    "    \n",
    "    return {\n",
    "        \"rgb_0_255\": rgb_values,\n",
    "        \"rgb_0_1\": norm_values,\n",
    "        \"hex\": hex_values,\n",
    "        \"named\": named_colors,\n",
    "        \"matplotlib_cmap\": LinearSegmentedColormap.from_list(\"polar_cmap\", norm_values)\n",
    "    }\n",
    "\n",
    "\n",
    "# Get color palettes\n",
    "eccen_colors = get_eccentricity_palette()\n",
    "polar_colors = get_polar_palette()\n",
    "\n",
    "\n",
    "# Rotate axis\n",
    "def rotate_coords(coords, axis, angle_degrees):\n",
    "    \"\"\"\n",
    "    Rotates coordinates by a given angle around the specified axis.\n",
    "    \n",
    "    Parameters:\n",
    "        coords (np.ndarray): shape (3, N) (x, y, z as first dimension)\n",
    "        axis (str): 'x', 'y', or 'z'\n",
    "        angle_degrees (float): rotation angle in degrees\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: rotated coordinates, shape (3, N)\n",
    "    \"\"\"\n",
    "    theta = np.deg2rad(angle_degrees)\n",
    "    if axis == 'x':\n",
    "        rot = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, np.cos(theta), -np.sin(theta)],\n",
    "            [0, np.sin(theta),  np.cos(theta)]\n",
    "        ])\n",
    "    elif axis == 'y':\n",
    "        rot = np.array([\n",
    "            [ np.cos(theta), 0, np.sin(theta)],\n",
    "            [ 0,             1, 0            ],\n",
    "            [-np.sin(theta), 0, np.cos(theta)]\n",
    "        ])\n",
    "    elif axis == 'z':\n",
    "        rot = np.array([\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta),  np.cos(theta), 0],\n",
    "            [0,              0,             1]\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"axis must be 'x', 'y', or 'z'\")\n",
    "    return rot @ coords\n",
    "\n",
    "\n",
    "\n",
    "# Plotting function (adapted from original)\n",
    "def plot_and_save_brains(lh_map, rh_map, colormap, mesh_lh, mesh_rh, strips_lh, strips_rh, mask_lh, mask_rh, view, vmin=None, vmax=None, cbar_label='Value', cf_property='r2', polar_colormap='polar'):\n",
    "    \"\"\"\n",
    "    Plot brain surfaces with given maps and colormap, set the view based on the flag, and save to PNG.\n",
    "    \n",
    "    Parameters:\n",
    "    - lh_map: array-like, map data for left hemisphere\n",
    "    - rh_map: array-like, map data for right hemisphere\n",
    "    - colormap: matplotlib colormap object\n",
    "    - mesh_lh: mesh object for left hemisphere\n",
    "    - mesh_rh: mesh object for right hemisphere\n",
    "    - strips_lh: underlay data for left hemisphere\n",
    "    - strips_rh: underlay data for right hemisphere\n",
    "    - mask_lh: mask for left hemisphere\n",
    "    - mask_rh: mask for right hemisphere\n",
    "    - view: str ('ventral' or 'dorsal') or tuple (azim, elev, dist) to set the camera view\n",
    "    - vmin: float, minimum value for colormap scaling (optional)\n",
    "    - vmax: float, maximum value for colormap scaling (optional)\n",
    "    - cbar_label: str, label for the colorbar (optional, default: 'Value')\n",
    "    - cf_property: str, CF property being plotted (for specialized colorbar insets)\n",
    "    - polar_colormap: str, colormap type for polar angle ('polar' or 'hsv')\n",
    "    \"\"\"\n",
    "    if isinstance(view, tuple) and len(view) == 3:\n",
    "        azim, elev, dist = view\n",
    "    elif view == 'ventral':\n",
    "        azim, elev, dist = -172, -8, 180\n",
    "    elif view == 'dorsal':\n",
    "        azim, elev, dist = -6.13, 31.34, 46.26\n",
    "    else:\n",
    "        raise ValueError(\"view must be 'ventral', 'dorsal', or a tuple (azim, elev, dist)\")\n",
    "       \n",
    "    # Create figure with HD 720p size\n",
    "    fig = ipv.figure(width=1280, height=720)\n",
    "    \n",
    "    # Plot right hemisphere\n",
    "    ny.cortex_plot(mesh_rh, surface='inflated', color=rh_map, cmap=colormap,\n",
    "        underlay=strips_rh, underlay_cmap='gray', underlay_vmin=-5, underlay_vmax=0.0, mask=mask_rh,\n",
    "        vmin=vmin, vmax=vmax,\n",
    "        figure=fig)\n",
    "    \n",
    "    # Plot left hemisphere\n",
    "    ny.cortex_plot(mesh_lh, surface='inflated', color=lh_map, cmap=colormap,\n",
    "        underlay=strips_lh, underlay_cmap='gray', underlay_vmin=-5, underlay_vmax=0.0, mask=mask_lh,\n",
    "        vmin=vmin, vmax=vmax,\n",
    "        figure=fig)\n",
    "    \n",
    "    # Compute the center of the plot (mean of all mesh coordinates)\n",
    "    all_coords = np.concatenate([mesh_lh.coordinates, mesh_rh.coordinates], axis=1)\n",
    "    center = np.mean(all_coords, axis=1)\n",
    "    fig.camera.center = center\n",
    "    \n",
    "    # Custom function to set view relative to center\n",
    "    def set_view(fig, azimuth, elevation, distance):\n",
    "        center = fig.camera.center\n",
    "        elev_rad = np.radians(elevation)\n",
    "        az_rad = np.radians(azimuth)\n",
    "        unit = np.array([\n",
    "            np.cos(elev_rad) * np.cos(az_rad),\n",
    "            np.cos(elev_rad) * np.sin(az_rad),\n",
    "            np.sin(elev_rad)\n",
    "        ])\n",
    "        fig.camera.position = tuple(center + distance * unit)\n",
    "    \n",
    "    # Adjust the final view\n",
    "    set_view(fig, azim, elev, dist)\n",
    "\n",
    "    ipv.show()\n",
    "    \n",
    "    # Add colorbar using inset - property-specific\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import cm\n",
    "    from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "    from matplotlib.patches import Wedge\n",
    "    \n",
    "    # Normalize values\n",
    "    vmin_val = vmin if vmin is not None else np.nanmin([np.nanmin(lh_map), np.nanmin(rh_map)])\n",
    "    vmax_val = vmax if vmax is not None else np.nanmax([np.nanmax(lh_map), np.nanmax(rh_map)])\n",
    "    \n",
    "    if cf_property == 'eccentricity':\n",
    "        # Create radial concentric rings colorbar for eccentricity\n",
    "        fig_cb, ax_main = plt.subplots(figsize=(3, 3))\n",
    "        ax_main.set_aspect('equal')\n",
    "        ax_main.set_xlim(-1.5, 1.5)\n",
    "        ax_main.set_ylim(-1.5, 1.5)\n",
    "        ax_main.set_axis_off()\n",
    "        ax_main.text(0.5, -0.05, r'$\\mathit{r}\\ (\\mathrm{deg})$', ha='center', va='top', \n",
    "                    fontsize=14, transform=ax_main.transAxes)\n",
    "        \n",
    "        num_ecc_colors = len(eccen_colors[\"hex\"])\n",
    "        for i, color in enumerate(eccen_colors[\"hex\"]):\n",
    "            inner_r = i / num_ecc_colors\n",
    "            outer_r = (i + 1) / num_ecc_colors\n",
    "            ring = Wedge((0, 0), outer_r, 0, 360, width=outer_r - inner_r, color=color)\n",
    "            ax_main.add_patch(ring)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    elif cf_property in ('polar', 'polar_angle'):\n",
    "        # Create polar pie chart colorbar for polar angle\n",
    "        fig_cb, ax_main = plt.subplots(figsize=(3, 3))\n",
    "        ax_main.set_aspect('equal')\n",
    "        ax_main.set_axis_off()\n",
    "        \n",
    "        if polar_colormap == 'hsv':\n",
    "            # Use HSV colormap - create gradient pie chart\n",
    "            import matplotlib.pyplot as plt\n",
    "            n_segments = 20\n",
    "            theta = np.linspace(0, 2*np.pi, n_segments, endpoint=False)\n",
    "            colors_hsv = [plt.cm.hsv(i/n_segments) for i in range(n_segments)]\n",
    "            ax_main.pie([1]*n_segments, colors=colors_hsv, \n",
    "                       startangle=180, counterclock=False)\n",
    "        else:\n",
    "            # Use custom polar colormap\n",
    "            ax_main.pie([1]*len(polar_colors[\"hex\"]), colors=polar_colors[\"hex\"], \n",
    "                       startangle=180, counterclock=False)\n",
    "        \n",
    "        ax_main.text(0.5, -0.05, r'$\\theta\\ (\\mathrm{rad})$', ha='center', va='top', \n",
    "                    fontsize=14, transform=ax_main.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Create standard horizontal colorbar for other properties\n",
    "        fig_cb, ax_main = plt.subplots(figsize=(8, 2))\n",
    "        ax_main.set_axis_off()\n",
    "        \n",
    "        # Create inset for horizontal colorbar\n",
    "        cbar_inset = inset_axes(ax_main, width=\"70%\", height=\"30%\", loc=\"center\", borderpad=0)\n",
    "        \n",
    "        norm = plt.Normalize(vmin=vmin_val, vmax=vmax_val)\n",
    "        \n",
    "        # Create colorbar in inset\n",
    "        cb = plt.colorbar(cm.ScalarMappable(norm=norm, cmap=colormap),\n",
    "                          cax=cbar_inset, orientation='horizontal')\n",
    "        cb.set_label(cbar_label, fontsize=14)\n",
    "        cb.ax.tick_params(labelsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Create widgets for real-time updates\n",
    "    azimuth_widget = FloatText(description='Azimuth:', step=0.1, disabled=True)\n",
    "    elevation_widget = FloatText(description='Elevation:', step=0.1, disabled=True)\n",
    "    distance_widget = FloatText(description='Distance:', step=0.1, disabled=True)\n",
    "\n",
    "    set_view_widget = Textarea(\n",
    "        description='set_view call:',\n",
    "        value='set_view(fig, 0.00, 0.00, 0.00)',\n",
    "        disabled=True,\n",
    "        layout={'width': '400px', 'height': '50px'}\n",
    "    )\n",
    "\n",
    "    def update_widgets(change):\n",
    "        pos = fig.camera.position\n",
    "        center = fig.camera.center\n",
    "        v = np.array(pos) - np.array(center)\n",
    "        dist = np.linalg.norm(v)\n",
    "        if dist > 0:\n",
    "            elevation = np.degrees(np.arcsin(v[2] / dist))\n",
    "            azimuth = np.degrees(np.arctan2(v[1], v[0]))\n",
    "        else:\n",
    "            azimuth = 0\n",
    "            elevation = 0\n",
    "        distance = dist\n",
    "        azimuth_widget.value = azimuth\n",
    "        elevation_widget.value = elevation\n",
    "        distance_widget.value = distance\n",
    "        set_view_widget.value = f\"set_view(fig, {azimuth:.2f}, {elevation:.2f}, {distance:.2f})\"\n",
    "        print(set_view_widget.value)\n",
    "\n",
    "    fig.camera.observe(update_widgets, names=['position'])\n",
    "    update_widgets(None)\n",
    "\n",
    "    # Display the widgets\n",
    "    from IPython.display import display\n",
    "    display(VBox([HBox([azimuth_widget, elevation_widget, distance_widget]), set_view_widget]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76b2f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d69958347a41a39bc84795c5db7e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Dataset:', options=(('Le Petit Prince (7T)'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive CF Results Plotting\n",
    "# Set anterior_threshold to None for full whole brain, or a value (e.g., -30) for posterior cut\n",
    "anterior_threshold = None\n",
    "from ipywidgets import Dropdown, FloatSlider, interact\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Get the current notebook's directory and define paths\n",
    "notebook_dir = Path().resolve()\n",
    "\n",
    "# Dataset configurations\n",
    "DATASET_CONFIGS = {\n",
    "    'LPP7T': {\n",
    "        'name': 'Le Petit Prince (7T)',\n",
    "        'task_pattern': r'cf_results_sub-(\\d+)_(ses-\\d+)_(\\w+)_(lh|rh)-source_ecc([\\d.]+)-([\\d.]+)\\.npz',\n",
    "        'fs_subject_format': 'sub-{subject_id}_ses-01_iso'\n",
    "    },\n",
    "    'CB3T': {\n",
    "        'name': 'Congenital Blindness (3T)',\n",
    "        'task_pattern': r'cf_results_sub-(\\d+)_(ses-\\d+)_([\\w-]+)_(lh|rh)-source_ecc([\\d.]+)-([\\d.]+)\\.npz',\n",
    "        'fs_subject_format': 'sub-{subject_id}_ses-01_iso'\n",
    "    },\n",
    "    'iCRTX7T': {\n",
    "        'name': 'iCORTEX (7T)',\n",
    "        'task_pattern': r'cf_results_sub-(\\d+)_(ses-\\d+)_([\\w-]+)_(lh|rh)-source_ecc([\\d.]+)-([\\d.]+)\\.npz',\n",
    "        'fs_subject_format': 'sub-{subject_id}_ses-01_iso'\n",
    "    }\n",
    "}\n",
    "\n",
    "DATASET_CONFIGS['iCRTX7T']['has_prf'] = True  # Flag for pRF availability\n",
    "\n",
    "# Initialize with LPP7T dataset\n",
    "current_dataset_key = 'LPP7T'\n",
    "base_data_path = notebook_dir / 'data' / current_dataset_key\n",
    "\n",
    "if not base_data_path.exists():\n",
    "    print(f\"‚ö†Ô∏è No data found for {DATASET_CONFIGS[current_dataset_key]['name']}\")\n",
    "    print(f\"Expected path: {base_data_path}\")\n",
    "    print(\"\\nPlease ensure data is organized as:\")\n",
    "    print(\"  data/\")\n",
    "    print(\"    LPP7T/\")\n",
    "    print(\"      derivatives/cf-models/\")\n",
    "    print(\"      fs_subjects/\")\n",
    "    print(\"    CB3T/\")\n",
    "    print(\"      derivatives/cf-models/\")\n",
    "    print(\"      fs_subjects/\")\n",
    "\n",
    "# Define CF models directory and FreeSurfer subjects directory\n",
    "cf_models_dir = base_data_path / 'derivatives' / 'cf-models'\n",
    "fs_subjects_dir = base_data_path / 'fs_subjects'\n",
    "\n",
    "\n",
    "# Scan available CF model files\n",
    "def scan_cf_models(cf_models_dir, task_pattern):\n",
    "    \"\"\"Scan CF models directory and extract available subjects, tasks, and sources.\"\"\"\n",
    "    cf_models_dir = Path(cf_models_dir)\n",
    "    pattern = str(cf_models_dir / 'cf_results_sub-*.npz')\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    available_models = []\n",
    "    subjects = set()\n",
    "    tasks = set()\n",
    "    \n",
    "    for f in files:\n",
    "        basename = Path(f).name\n",
    "        # Use dataset-specific pattern\n",
    "        match = re.match(task_pattern, basename)\n",
    "        if match:\n",
    "            subject_id, session_id, task, source_hemi, min_ecc, max_ecc = match.groups()\n",
    "            subjects.add(subject_id)\n",
    "            tasks.add(task)\n",
    "            available_models.append({\n",
    "                'file': f,\n",
    "                'subject': subject_id,\n",
    "                'session': session_id,\n",
    "                'task': task,\n",
    "                'source_hemi': source_hemi,\n",
    "                'min_ecc': float(min_ecc),\n",
    "                'max_ecc': float(max_ecc)\n",
    "            })\n",
    "    \n",
    "    return sorted(list(subjects)), sorted(list(tasks)), available_models\n",
    "\n",
    "# Scan models with current dataset pattern\n",
    "available_subjects, available_tasks, cf_models_info = scan_cf_models(\n",
    "    cf_models_dir, \n",
    "    DATASET_CONFIGS[current_dataset_key]['task_pattern']\n",
    ")\n",
    "\n",
    "\n",
    "# Define property-specific defaults\n",
    "property_config = {\n",
    "    'r2': {\n",
    "        'adaptive': True,\n",
    "        'vmin': 0,\n",
    "        'vmax': 1\n",
    "    },\n",
    "    'polar': {\n",
    "        'adaptive': True,\n",
    "        'vmin': -np.pi,\n",
    "        'vmax': np.pi\n",
    "    },\n",
    "    'eccentricity': {\n",
    "        'adaptive': False,\n",
    "        'vmin': 0.5,\n",
    "        'vmax': 6.5\n",
    "    },\n",
    "    'cf_size': {\n",
    "        'adaptive': False,\n",
    "        'vmin': 0.5,\n",
    "        'vmax': 5.0\n",
    "    },\n",
    "    'polar_angle': {\n",
    "        'adaptive': True,\n",
    "        'vmin': -np.pi,\n",
    "        'vmax': np.pi\n",
    "    },\n",
    "    'size': {\n",
    "        'adaptive': False,\n",
    "        'vmin': 0,\n",
    "        'vmax': 5.0\n",
    "    },\n",
    "    'baseline': {\n",
    "        'adaptive': True,\n",
    "        'vmin': 0,\n",
    "        'vmax': 1\n",
    "    },\n",
    "    'amplitude': {\n",
    "        'adaptive': True,\n",
    "        'vmin': 0,\n",
    "        'vmax': 1\n",
    "    },\n",
    "    'hrf_delay': {\n",
    "        'adaptive': True,\n",
    "        'vmin': 0,\n",
    "        'vmax': 1\n",
    "    },\n",
    "    'hrf_dispersion': {\n",
    "        'adaptive': True,\n",
    "        'vmin': 0,\n",
    "        'vmax': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cache for loaded results (includes dataset key to avoid conflicts)\n",
    "loaded_results_cache = {}\n",
    "\n",
    "def load_cf_results(subject_id, task, source_hemi, dataset_key='LPP7T'):\n",
    "    \"\"\"Load CF results for given subject, task, and source hemisphere.\"\"\"\n",
    "    # Create cache key with dataset prefix\n",
    "    cache_key = f\"{dataset_key}_sub-{subject_id}_{task}_{source_hemi}\"\n",
    "    \n",
    "    # Check cache\n",
    "    if cache_key in loaded_results_cache:\n",
    "        return loaded_results_cache[cache_key]\n",
    "    \n",
    "    # Find matching file\n",
    "    matching_files = [m for m in cf_models_info \n",
    "                     if m['subject'] == subject_id \n",
    "                     and m['task'] == task \n",
    "                     and m['source_hemi'] == source_hemi]\n",
    "    \n",
    "    if not matching_files:\n",
    "        print(f\"‚ùå No model found for sub-{subject_id}, task={task}, source={source_hemi}\")\n",
    "        return None, None\n",
    "    \n",
    "    model_file = matching_files[0]['file']\n",
    "        \n",
    "    # Load results\n",
    "    results = np.load(model_file, allow_pickle=True)\n",
    "    results_lh = results['lh_results'].item()\n",
    "    results_rh = results['rh_results'].item()\n",
    "    \n",
    "    # Cache results\n",
    "    loaded_results_cache[cache_key] = (results_lh, results_rh)\n",
    "    \n",
    "    return results_lh, results_rh\n",
    "\n",
    "\n",
    "def load_prf_results(subject_id, dataset_key='iCRTX7T'):\n",
    "    \"\"\"Load pRF results from CSV files for given subject.\n",
    "\n",
    "    Returns (lh_results, rh_results) as dicts where values are lists of (vertex_index, value) tuples.\n",
    "    Arrays will be created later with correct mesh sizes.\n",
    "    \"\"\"\n",
    "    cache_key = f\"prf_{dataset_key}_sub-{subject_id}\"\n",
    "\n",
    "    if cache_key in loaded_results_cache:\n",
    "        return loaded_results_cache[cache_key]\n",
    "\n",
    "    dataset_root = notebook_dir / 'data' / dataset_key\n",
    "\n",
    "    # Direct path for pRF file\n",
    "    prf_file = dataset_root / 'derivatives' / 'pRF-maps' / f'pRF_parameters_both_hemispheres_averaged_sub-{subject_id}_ses-01.csv'\n",
    "\n",
    "    prf_file_lh = dataset_root / 'derivatives' / 'pRF-maps' / f'pRF_parameters_lh_sub-{subject_id}_ses-01.csv'\n",
    "    prf_file_rh = dataset_root / 'derivatives' / 'pRF-maps' / f'pRF_parameters_rh_sub-{subject_id}_ses-01.csv'\n",
    "\n",
    "    # Empty results as dicts of empty lists\n",
    "    empty_lh = {\n",
    "        'polar_angle': [],\n",
    "        'eccentricity': [],\n",
    "        'size': [],\n",
    "        'baseline': [],\n",
    "        'amplitude': [],\n",
    "        'hrf_delay': [],\n",
    "        'hrf_dispersion': [],\n",
    "        'r2': []\n",
    "    }\n",
    "\n",
    "    empty_rh = {\n",
    "        'polar_angle': [],\n",
    "        'eccentricity': [],\n",
    "        'size': [],\n",
    "        'baseline': [],\n",
    "        'amplitude': [],\n",
    "        'hrf_delay': [],\n",
    "        'hrf_dispersion': [],\n",
    "        'r2': []\n",
    "    }\n",
    "\n",
    "    if not prf_file.exists():\n",
    "        print(f\"‚ö†Ô∏è pRF file not found: {prf_file}\")\n",
    "        loaded_results_cache[cache_key] = (empty_lh, empty_rh)\n",
    "        return empty_lh, empty_rh\n",
    "\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(prf_file, sep=',', engine='python')\n",
    "    \n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    hemi_col = cols_lower.get('hemisphere') or cols_lower.get('hemi')\n",
    "    vertex_col = cols_lower.get('vertex_index') or cols_lower.get('vertex')\n",
    "    pa_col = cols_lower.get('polar_angle') or cols_lower.get('polar')\n",
    "    ecc_col = cols_lower.get('eccentricity')\n",
    "    size_col = cols_lower.get('sd')\n",
    "    r2_col = cols_lower.get('r2')\n",
    "    baseline_col = cols_lower.get('baseline')\n",
    "    amplitude_col = cols_lower.get('amplitude')\n",
    "    hrf_delay_col = cols_lower.get('hrf_delay')\n",
    "    hrf_dispersion_col = cols_lower.get('hrf_dispersion')\n",
    "\n",
    "    # If hemisphere column missing, try to infer from filename\n",
    "    hemi_default = None\n",
    "    if hemi_col is None:\n",
    "        fname = prf_file.name.lower()\n",
    "        if '_lh' in fname or '.lh' in fname or 'lh_' in fname:\n",
    "            hemi_default = 'lh'\n",
    "        elif '_rh' in fname or '.rh' in fname or 'rh_' in fname:\n",
    "            hemi_default = 'rh'\n",
    "        elif 'both' in fname:\n",
    "            print(f\"‚ö†Ô∏è pRF CSV appears to be 'both' but lacks a hemisphere column: {prf_file.name}\")\n",
    "            loaded_results_cache[cache_key] = (empty_lh, empty_rh)\n",
    "            return empty_lh, empty_rh\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è pRF CSV missing 'hemisphere' column and cannot infer hemisphere from filename: {prf_file.name}\")\n",
    "            loaded_results_cache[cache_key] = (empty_lh, empty_rh)\n",
    "            return empty_lh, empty_rh\n",
    "\n",
    "    lh_results = {\n",
    "        'polar_angle': [],\n",
    "        'eccentricity': [],\n",
    "        'size': [],\n",
    "        'baseline': [],\n",
    "        'amplitude': [],\n",
    "        'hrf_delay': [],\n",
    "        'hrf_dispersion': [],\n",
    "        'r2': []\n",
    "    }\n",
    "\n",
    "    rh_results = {\n",
    "        'polar_angle': [],\n",
    "        'eccentricity': [],\n",
    "        'size': [],\n",
    "        'baseline': [],\n",
    "        'amplitude': [],\n",
    "        'hrf_delay': [],\n",
    "        'hrf_dispersion': [],\n",
    "        'r2': []\n",
    "    }\n",
    "\n",
    "    # Iterate rows and collect data\n",
    "    for _, row in df.iterrows():\n",
    "        if hemi_col is not None:\n",
    "            hemi = str(row[hemi_col]).lower()\n",
    "        else:\n",
    "            hemi = hemi_default\n",
    "\n",
    "        try:\n",
    "            vidx = int(row[vertex_col])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if hemi.startswith('lh'):\n",
    "            if pa_col and pa_col in row and not pd.isna(row[pa_col]):\n",
    "                lh_results['polar_angle'].append((vidx, row[pa_col]))\n",
    "            if ecc_col and ecc_col in row and not pd.isna(row[ecc_col]):\n",
    "                lh_results['eccentricity'].append((vidx, row[ecc_col]))\n",
    "            if size_col and size_col in row and not pd.isna(row[size_col]):\n",
    "                lh_results['size'].append((vidx, row[size_col]))\n",
    "            if r2_col and r2_col in row and not pd.isna(row[r2_col]):\n",
    "                lh_results['r2'].append((vidx, row[r2_col]))\n",
    "            if baseline_col and baseline_col in row and not pd.isna(row[baseline_col]):\n",
    "                lh_results['baseline'].append((vidx, row[baseline_col]))\n",
    "            if amplitude_col and amplitude_col in row and not pd.isna(row[amplitude_col]):\n",
    "                lh_results['amplitude'].append((vidx, row[amplitude_col]))\n",
    "            if hrf_delay_col and hrf_delay_col in row and not pd.isna(row[hrf_delay_col]):\n",
    "                lh_results['hrf_delay'].append((vidx, row[hrf_delay_col]))\n",
    "            if hrf_dispersion_col and hrf_dispersion_col in row and not pd.isna(row[hrf_dispersion_col]):\n",
    "                lh_results['hrf_dispersion'].append((vidx, row[hrf_dispersion_col]))\n",
    "        elif hemi.startswith('rh'):\n",
    "            if pa_col and pa_col in row and not pd.isna(row[pa_col]):\n",
    "                rh_results['polar_angle'].append((vidx, row[pa_col]))\n",
    "            if ecc_col and ecc_col in row and not pd.isna(row[ecc_col]):\n",
    "                rh_results['eccentricity'].append((vidx, row[ecc_col]))\n",
    "            if size_col and size_col in row and not pd.isna(row[size_col]):\n",
    "                rh_results['size'].append((vidx, row[size_col]))\n",
    "            if r2_col and r2_col in row and not pd.isna(row[r2_col]):\n",
    "                rh_results['r2'].append((vidx, row[r2_col]))\n",
    "            if baseline_col and baseline_col in row and not pd.isna(row[baseline_col]):\n",
    "                rh_results['baseline'].append((vidx, row[baseline_col]))\n",
    "            if amplitude_col and amplitude_col in row and not pd.isna(row[amplitude_col]):\n",
    "                rh_results['amplitude'].append((vidx, row[amplitude_col]))\n",
    "            if hrf_delay_col and hrf_delay_col in row and not pd.isna(row[hrf_delay_col]):\n",
    "                rh_results['hrf_delay'].append((vidx, row[hrf_delay_col]))\n",
    "            if hrf_dispersion_col and hrf_dispersion_col in row and not pd.isna(row[hrf_dispersion_col]):\n",
    "                rh_results['hrf_dispersion'].append((vidx, row[hrf_dispersion_col]))\n",
    "\n",
    "    loaded_results_cache[cache_key] = (lh_results, rh_results)\n",
    "    return lh_results, rh_results\n",
    "\n",
    "def update_plot(dataset_key, subject_id, task, source_hemi, map_type, cf_property,\n",
    "                r2_threshold, use_adaptive_range, vmin, vmax, polar_colormap='polar',\n",
    "                hemi_separation=80):\n",
    "    \"\"\"Update surface plot based on widget selections (supports CF and pRF).\"\"\"\n",
    "    # Declare globals at the start\n",
    "    global current_loaded_subject, current_loaded_dataset, lh_mesh, rh_mesh, lh_curv_map, rh_curv_map\n",
    "    \n",
    "    try:\n",
    "        if map_type == 'CF':\n",
    "            results_lh, results_rh = load_cf_results(subject_id, task, source_hemi, dataset_key)\n",
    "            prop_map = {\n",
    "                'eccentricity': 'inherited_eccen',\n",
    "                'polar': 'inherited_polar',\n",
    "                'cf_size': 'cf_size',\n",
    "                'r2': 'r2'\n",
    "            }\n",
    "            if results_lh is None or results_rh is None:\n",
    "                return\n",
    "            if cf_property not in prop_map:\n",
    "                print(f\"Unknown CF property: {cf_property}\")\n",
    "                return\n",
    "            lh_data = results_lh[prop_map[cf_property]].copy()\n",
    "            rh_data = results_rh[prop_map[cf_property]].copy()\n",
    "            lh_r2 = results_lh.get('r2', np.full_like(lh_data, np.nan))\n",
    "            rh_r2 = results_rh.get('r2', np.full_like(rh_data, np.nan))\n",
    "        elif map_type == 'pRF':\n",
    "            results_lh, results_rh = load_prf_results(subject_id, dataset_key)\n",
    "            prop_map_prf = {\n",
    "                'eccentricity': 'eccentricity',\n",
    "                'polar_angle': 'polar_angle',\n",
    "                'size': 'size',\n",
    "                'baseline': 'baseline',\n",
    "                'amplitude': 'amplitude',\n",
    "                'hrf_delay': 'hrf_delay',\n",
    "                'hrf_dispersion': 'hrf_dispersion',\n",
    "                'r2': 'r2'\n",
    "            }\n",
    "            if results_lh is None or results_rh is None:\n",
    "                return\n",
    "            if cf_property not in prop_map_prf:\n",
    "                print(f\"Unknown pRF property: {cf_property}\")\n",
    "                return\n",
    "            \n",
    "            # For pRF, load mesh if needed to get vertex counts for arrays\n",
    "            current_sub_id = f\"{int(subject_id):02d}\"\n",
    "            need_load = (\n",
    "                'current_loaded_subject' not in globals() or\n",
    "                'current_loaded_dataset' not in globals() or\n",
    "                current_loaded_subject != current_sub_id or\n",
    "                current_loaded_dataset != dataset_key\n",
    "            )\n",
    "            if need_load:\n",
    "                fs_subject_format = DATASET_CONFIGS[dataset_key]['fs_subject_format']\n",
    "                fs_subject_name = fs_subject_format.format(subject_id=current_sub_id)\n",
    "                fs_subject_path = fs_subjects_dir / fs_subject_name\n",
    "                print(f\"üß† Loading mesh for {DATASET_CONFIGS[dataset_key]['name']}: {fs_subject_name}\")\n",
    "                if not fs_subject_path.exists():\n",
    "                    print(f\"‚ùå FreeSurfer subject not found: {fs_subject_path}\")\n",
    "                    return\n",
    "                try:\n",
    "                    import nibabel.freesurfer as fs\n",
    "                    surf_dir = fs_subject_path / 'surf'\n",
    "                    lh_coords, lh_faces = fs.read_geometry(str(surf_dir / 'lh.inflated'))\n",
    "                    rh_coords, rh_faces = fs.read_geometry(str(surf_dir / 'rh.inflated'))\n",
    "                    lh_mesh = Mesh(Tesselation(lh_faces.T), lh_coords.T)\n",
    "                    rh_mesh = Mesh(Tesselation(rh_faces.T), rh_coords.T)\n",
    "                    lh_curv_map = fs.read_morph_data(str(surf_dir / 'lh.curv'))\n",
    "                    rh_curv_map = fs.read_morph_data(str(surf_dir / 'rh.curv'))\n",
    "                    current_loaded_subject = current_sub_id\n",
    "                    current_loaded_dataset = dataset_key\n",
    "                    print(f\"‚úì Loaded mesh: LH={lh_coords.shape[0]} vertices, RH={rh_coords.shape[0]} vertices\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading mesh: {e}\")\n",
    "                    return\n",
    "            \n",
    "            # Create arrays for pRF data with correct mesh sizes (use global mesh shape)\n",
    "            lh_data = np.full(lh_mesh.coordinates.shape[1], np.nan)\n",
    "            rh_data = np.full(rh_mesh.coordinates.shape[1], np.nan)\n",
    "            lh_r2 = np.full(lh_mesh.coordinates.shape[1], np.nan)\n",
    "            rh_r2 = np.full(rh_mesh.coordinates.shape[1], np.nan)\n",
    "            \n",
    "            # Fill LH data (apply transformation for size)\n",
    "            for vidx, val in results_lh[prop_map_prf[cf_property]]:\n",
    "                if 0 <= vidx < lh_mesh.coordinates.shape[1]:\n",
    "                    if cf_property == 'size':\n",
    "                        lh_data[vidx] = abs(val) * 2.355\n",
    "                    else:\n",
    "                        lh_data[vidx] = val\n",
    "            for vidx, val in results_lh['r2']:\n",
    "                if 0 <= vidx < lh_mesh.coordinates.shape[1]:\n",
    "                    lh_r2[vidx] = val\n",
    "            \n",
    "            # Fill RH data (apply transformation for size)\n",
    "            for vidx, val in results_rh[prop_map_prf[cf_property]]:\n",
    "                if 0 <= vidx < rh_mesh.coordinates.shape[1]:\n",
    "                    if cf_property == 'size':\n",
    "                        rh_data[vidx] = abs(val) * 2.355\n",
    "                    else:\n",
    "                        rh_data[vidx] = val\n",
    "            for vidx, val in results_rh['r2']:\n",
    "                if 0 <= vidx < rh_mesh.coordinates.shape[1]:\n",
    "                    rh_r2[vidx] = val\n",
    "            \n",
    "            # pRF doesn't need task/source\n",
    "            task = 'averaged'\n",
    "            source_hemi = 'both'\n",
    "\n",
    "        else:\n",
    "            print(f\"Unknown map_type: {map_type}\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading results: {e}\")\n",
    "        return\n",
    "\n",
    "    # Apply R¬≤ mask (now works for both CF and pRF since arrays are ready)\n",
    "    lh_mask = np.isnan(lh_r2) | (lh_r2 < r2_threshold)\n",
    "    rh_mask = np.isnan(rh_r2) | (rh_r2 < r2_threshold)\n",
    "    lh_data = lh_data.copy()\n",
    "    rh_data = rh_data.copy()\n",
    "    lh_data[lh_mask] = np.nan\n",
    "    rh_data[rh_mask] = np.nan\n",
    "\n",
    "    # Load meshes if needed (unified for both CF and pRF; skips if already loaded)\n",
    "    current_sub_id = f\"{int(subject_id):02d}\"\n",
    "    need_load = (\n",
    "        'current_loaded_subject' not in globals() or\n",
    "        'current_loaded_dataset' not in globals() or\n",
    "        current_loaded_subject != current_sub_id or\n",
    "        current_loaded_dataset != dataset_key\n",
    "    )\n",
    "    if need_load:\n",
    "        fs_subject_format = DATASET_CONFIGS[dataset_key]['fs_subject_format']\n",
    "        fs_subject_name = fs_subject_format.format(subject_id=current_sub_id)\n",
    "        fs_subject_path = fs_subjects_dir / fs_subject_name\n",
    "        print(f\"üß† Loading mesh for {DATASET_CONFIGS[dataset_key]['name']}: {fs_subject_name}\")\n",
    "        if not fs_subject_path.exists():\n",
    "            print(f\"‚ùå FreeSurfer subject not found: {fs_subject_path}\")\n",
    "            return\n",
    "        try:\n",
    "            import nibabel.freesurfer as fs\n",
    "            surf_dir = fs_subject_path / 'surf'\n",
    "            lh_coords, lh_faces = fs.read_geometry(str(surf_dir / 'lh.inflated'))\n",
    "            rh_coords, rh_faces = fs.read_geometry(str(surf_dir / 'rh.inflated'))\n",
    "            lh_mesh = Mesh(Tesselation(lh_faces.T), lh_coords.T)\n",
    "            rh_mesh = Mesh(Tesselation(rh_faces.T), rh_coords.T)\n",
    "            lh_curv_map = fs.read_morph_data(str(surf_dir / 'lh.curv'))\n",
    "            rh_curv_map = fs.read_morph_data(str(surf_dir / 'rh.curv'))\n",
    "            current_loaded_subject = current_sub_id\n",
    "            current_loaded_dataset = dataset_key\n",
    "            print(f\"‚úì Loaded mesh: LH={lh_coords.shape[0]} vertices, RH={rh_coords.shape[0]} vertices\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading mesh: {e}\")\n",
    "            return\n",
    "\n",
    "    # Adaptive vmin/vmax\n",
    "    if use_adaptive_range:\n",
    "        valid = np.concatenate([lh_data[~np.isnan(lh_data)], rh_data[~np.isnan(rh_data)]])\n",
    "        if valid.size > 0:\n",
    "            vmin = np.percentile(valid, 2)\n",
    "            vmax = np.percentile(valid, 98)\n",
    "        else:\n",
    "            cfg = property_config.get(cf_property, {'vmin': 0, 'vmax': 1})\n",
    "            vmin, vmax = cfg['vmin'], cfg['vmax']\n",
    "\n",
    "    # Prepare strips and masks\n",
    "    lh_strips_plot = lh_curv_map.astype(float)\n",
    "    lh_strips_plot[lh_strips_plot > 0] = np.nan\n",
    "    rh_strips_plot = rh_curv_map.astype(float)\n",
    "    rh_strips_plot[rh_strips_plot > 0] = np.nan\n",
    "    lh_mask_plot = ~np.isnan(lh_data)\n",
    "    rh_mask_plot = ~np.isnan(rh_data)\n",
    "\n",
    "    # Apply rotations and separation\n",
    "    angle = 0\n",
    "    lh_coords_plot = lh_mesh.coordinates\n",
    "    rh_coords_plot = rh_mesh.coordinates\n",
    "    lh_faces_plot = lh_mesh.tess.faces\n",
    "    rh_faces_plot = rh_mesh.tess.faces\n",
    "    lh_coords_medial = rotate_coords(lh_coords_plot, axis='z', angle_degrees=-angle)\n",
    "    rh_coords_medial = rotate_coords(rh_coords_plot, axis='z', angle_degrees=angle * 2)\n",
    "    rh_coords_medial[0, :] += hemi_separation\n",
    "    lh_mesh_shifted = Mesh(Tesselation(lh_faces_plot), lh_coords_medial)\n",
    "    rh_mesh_shifted = Mesh(Tesselation(rh_faces_plot), rh_coords_medial)\n",
    "\n",
    "    # Select colormap and label\n",
    "    if cf_property == 'eccentricity':\n",
    "        grad_cmap = eccen_colors['matplotlib_cmap']\n",
    "        cbar_label = r'Eccentricity $r$ (deg)'\n",
    "    elif cf_property in ('polar', 'polar_angle'):\n",
    "        grad_cmap = polar_colors['matplotlib_cmap'] if polar_colormap == 'polar' else plt.colormaps['hsv']\n",
    "        cbar_label = r'Polar angle $\\theta$ (rad)'\n",
    "    elif cf_property == 'r2':\n",
    "        grad_cmap = plt.colormaps['viridis']\n",
    "        cbar_label = r'$R^2$'\n",
    "    elif cf_property == 'cf_size':\n",
    "        grad_cmap = plt.colormaps['viridis']\n",
    "        cbar_label = r'CF size $\\sigma$ (mm)'\n",
    "    elif cf_property == 'size':\n",
    "        grad_cmap = plt.colormaps['viridis']\n",
    "        cbar_label = r'pRF size $\\sigma$ (deg)'\n",
    "    elif cf_property == 'baseline':\n",
    "        grad_cmap = plt.colormaps['hot']\n",
    "        cbar_label = 'Baseline (a.u.)'\n",
    "    elif cf_property == 'amplitude':\n",
    "        grad_cmap = plt.colormaps['viridis']\n",
    "        cbar_label = 'Amplitude (a.u.)'\n",
    "    elif cf_property == 'hrf_delay':\n",
    "        grad_cmap = plt.colormaps['viridis']\n",
    "        cbar_label = r'HRF delay ($s$)'\n",
    "    elif cf_property == 'hrf_dispersion':\n",
    "        grad_cmap = plt.colormaps['viridis']\n",
    "        cbar_label = 'HRF dispersion'\n",
    "    else:\n",
    "        grad_cmap = plt.colormaps['viridis']\n",
    "        cbar_label = 'Value'\n",
    "\n",
    "    # Plot\n",
    "    try:\n",
    "        plot_and_save_brains(lh_data, rh_data, grad_cmap,\n",
    "                             lh_mesh_shifted, rh_mesh_shifted,\n",
    "                             lh_strips_plot, rh_strips_plot,\n",
    "                             lh_mask_plot, rh_mask_plot,\n",
    "                             (-122, -27, 80), vmin=vmin, vmax=vmax,\n",
    "                             cbar_label=cbar_label, cf_property=cf_property,\n",
    "                             polar_colormap=polar_colormap)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error plotting brains: {e}\")\n",
    "        return\n",
    "\n",
    "    # Save current plot state\n",
    "    global current_plot_data\n",
    "    current_plot_data = {\n",
    "        'lh_data': lh_data,\n",
    "        'rh_data': rh_data,\n",
    "        'lh_mesh': lh_mesh_shifted,\n",
    "        'rh_mesh': rh_mesh_shifted,\n",
    "        'colormap': grad_cmap,\n",
    "        'vmin': vmin,\n",
    "        'vmax': vmax,\n",
    "        'cbar_label': cbar_label,\n",
    "        'subject_id': subject_id,\n",
    "        'task': task,\n",
    "        'source_hemi': source_hemi,\n",
    "        'cf_property': cf_property,\n",
    "        'r2_threshold': r2_threshold,\n",
    "        'map_type': map_type\n",
    "    }\n",
    "\n",
    "# Create widgets\n",
    "dataset_widget = Dropdown(\n",
    "    options=[(config['name'], key) for key, config in DATASET_CONFIGS.items()],\n",
    "    value='LPP7T',\n",
    "    description='Dataset:'\n",
    ")\n",
    "subject_widget = Dropdown(options=available_subjects, \n",
    "                         value=available_subjects[0] if available_subjects else '01', \n",
    "                         description='Subject:')\n",
    "task_widget = Dropdown(options=available_tasks, \n",
    "                      value=available_tasks[0] if available_tasks else 'LPP1', \n",
    "                      description='Task:')\n",
    "source_hemi_widget = Dropdown(options=['lh', 'rh'], value='lh', description='Source:')\n",
    "map_type_widget = Dropdown(options=['CF', 'pRF'], value='CF', description='Map type:')\n",
    "cf_property_widget = Dropdown(options=['eccentricity', 'polar', 'cf_size', 'r2'], \n",
    "                              value='eccentricity', description='Property:')\n",
    "r2_threshold_widget = FloatSlider(value=0.1, min=0.0, max=1.0, step=0.01, description='R¬≤ threshold:')\n",
    "\n",
    "# Adaptive range widget - default depends on CF property\n",
    "adaptive_range_widget = Dropdown(options=[True, False], value=False, \n",
    "                                description='Range:')\n",
    "\n",
    "# Min/max widgets - defaults depend on CF property - initialized for eccentricity\n",
    "vmin_widget = FloatSlider(value=0.5, min=-10, max=10, step=0.01, description='min:')\n",
    "vmax_widget = FloatSlider(value=6.5, min=-10, max=10, step=0.01, description='max:')\n",
    "\n",
    "# Polar colormap widget - only visible when plotting polar angle\n",
    "polar_colormap_widget = Dropdown(options=['polar', 'hsv'], value='polar', \n",
    "                                 description='Polar cmap:')\n",
    "\n",
    "# Hemisphere separation slider\n",
    "hemi_separation_widget = FloatSlider(\n",
    "    value=80, \n",
    "    min=0, \n",
    "    max=200, \n",
    "    step=1, \n",
    "    description='Hemi gap:',\n",
    "    tooltip='Distance between left and right hemispheres'\n",
    ")\n",
    "\n",
    "# Function to update widget defaults when CF property changes\n",
    "def update_widget_defaults(cf_property):\n",
    "    \"\"\"Update adaptive range and min/max defaults based on CF property and map type.\"\"\"\n",
    "    map_type = map_type_widget.value  # Get current map type\n",
    "    \n",
    "    if map_type == 'pRF' and cf_property == 'eccentricity':\n",
    "        # Special case for pRF eccentricity: set vmax to 4.5\n",
    "        adaptive_range_widget.value = False\n",
    "        vmin_widget.value = 0\n",
    "        vmax_widget.value = 5\n",
    "    else:\n",
    "        # Use default config for other properties\n",
    "        config = property_config.get(cf_property, {'adaptive': True, 'vmin': 0, 'vmax': 1})\n",
    "        adaptive_range_widget.value = config['adaptive']\n",
    "        vmin_widget.value = config['vmin']\n",
    "        vmax_widget.value = config['vmax']\n",
    "\n",
    "# Function to update property options and widget visibility based on map type\n",
    "def update_map_type_options(change):\n",
    "    \"\"\"Update property options and widget visibility when map type changes.\"\"\"\n",
    "    map_type = change['new']\n",
    "    if map_type == 'CF':\n",
    "        cf_property_widget.options = ['eccentricity', 'polar', 'cf_size', 'r2']\n",
    "        cf_property_widget.value = 'eccentricity'\n",
    "        task_widget.layout.display = 'flex'\n",
    "        source_hemi_widget.layout.display = 'flex'\n",
    "        cf_property_widget.description = 'CF parameter:'\n",
    "    elif map_type == 'pRF':\n",
    "        cf_property_widget.options = ['eccentricity', 'polar_angle', 'size', 'baseline', 'amplitude', 'hrf_delay', 'hrf_dispersion', 'r2']\n",
    "        cf_property_widget.value = 'eccentricity'\n",
    "        task_widget.layout.display = 'none'\n",
    "        source_hemi_widget.layout.display = 'none'\n",
    "        cf_property_widget.description = 'pRF parameter:'\n",
    "    # Update defaults for the new property\n",
    "    update_widget_defaults(cf_property_widget.value)\n",
    "\n",
    "# Function to update available subjects and tasks when dataset changes\n",
    "def update_dataset_options(change):\n",
    "    \"\"\"Update subject and task options when dataset changes.\"\"\"\n",
    "    global available_subjects, available_tasks, cf_models_info, base_data_path, cf_models_dir, fs_subjects_dir, current_dataset_key\n",
    "    \n",
    "    new_dataset_key = change['new']\n",
    "    current_dataset_key = new_dataset_key\n",
    "    \n",
    "    # Update base path to new dataset folder\n",
    "    base_data_path = notebook_dir / 'data' / new_dataset_key\n",
    "    \n",
    "    if not base_data_path.exists():\n",
    "        print(f\"‚ö†Ô∏è No data found for {DATASET_CONFIGS[new_dataset_key]['name']}\")\n",
    "        print(f\"Expected path: {base_data_path}\")\n",
    "        return\n",
    "    \n",
    "    # Update paths\n",
    "    cf_models_dir = base_data_path / 'derivatives' / 'cf-models'\n",
    "    fs_subjects_dir = base_data_path / 'fs_subjects'\n",
    "    \n",
    "    # Rescan with new pattern\n",
    "    task_pattern = DATASET_CONFIGS[new_dataset_key]['task_pattern']\n",
    "    available_subjects, available_tasks, cf_models_info = scan_cf_models(cf_models_dir, task_pattern)\n",
    "    \n",
    "    # Update widget options\n",
    "    subject_widget.options = available_subjects\n",
    "    subject_widget.value = available_subjects[0] if available_subjects else '01'\n",
    "    task_widget.options = available_tasks\n",
    "    task_widget.value = available_tasks[0] if available_tasks else 'LPP1'\n",
    "    \n",
    "    # Update map type options based on pRF availability\n",
    "    has_prf = DATASET_CONFIGS[new_dataset_key].get('has_prf', False)\n",
    "    if has_prf:\n",
    "        map_type_widget.options = ['CF', 'pRF']\n",
    "        map_type_widget.value = 'CF'  # Default to CF\n",
    "    else:\n",
    "        map_type_widget.options = ['CF']\n",
    "        map_type_widget.value = 'CF'\n",
    "    \n",
    "    print(f\"‚úì Switched to {DATASET_CONFIGS[new_dataset_key]['name']}\")\n",
    "    print(f\"  Found {len(available_subjects)} subjects, {len(available_tasks)} tasks\")\n",
    "    if has_prf:\n",
    "        print(\"  pRF data available\")\n",
    "\n",
    "# Link dataset widget to update function\n",
    "dataset_widget.observe(update_dataset_options, names='value')\n",
    "\n",
    "# Link map type widget to update function\n",
    "map_type_widget.observe(update_map_type_options, names='value')\n",
    "\n",
    "# Link CF property widget to update defaults\n",
    "cf_property_widget.observe(lambda change: update_widget_defaults(change['new']), names='value')\n",
    "\n",
    "# Interactive plot with conditional polar colormap widget\n",
    "from ipywidgets import interactive_output, VBox, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Create interactive output\n",
    "ui_controls = {\n",
    "    'dataset_key': dataset_widget,\n",
    "    'subject_id': subject_widget,\n",
    "    'task': task_widget, \n",
    "    'source_hemi': source_hemi_widget,\n",
    "    'map_type': map_type_widget,\n",
    "    'cf_property': cf_property_widget,\n",
    "    'r2_threshold': r2_threshold_widget,\n",
    "    'use_adaptive_range': adaptive_range_widget,\n",
    "    'vmin': vmin_widget,\n",
    "    'vmax': vmax_widget,\n",
    "    'polar_colormap': polar_colormap_widget,\n",
    "    'hemi_separation': hemi_separation_widget\n",
    "}\n",
    "\n",
    "out = interactive_output(update_plot, ui_controls)\n",
    "\n",
    "# Function to update widget visibility based on CF property\n",
    "def update_widget_visibility(change):\n",
    "    \"\"\"Show polar colormap widget only when polar angle is selected.\"\"\"\n",
    "    if change['new'] == 'polar':\n",
    "        polar_colormap_widget.layout.display = 'flex'\n",
    "    else:\n",
    "        polar_colormap_widget.layout.display = 'none'\n",
    "\n",
    "# Initially hide polar colormap widget if not plotting polar\n",
    "if cf_property_widget.value != 'polar':\n",
    "    polar_colormap_widget.layout.display = 'none'\n",
    "\n",
    "# Initially set task/source visibility for CF\n",
    "task_widget.layout.display = 'flex'\n",
    "source_hemi_widget.layout.display = 'flex'\n",
    "\n",
    "# Link visibility to CF property changes\n",
    "cf_property_widget.observe(update_widget_visibility, names='value')\n",
    "\n",
    "# Display widgets and output in compact 2-column layout\n",
    "display(VBox([\n",
    "    HBox([\n",
    "        VBox([dataset_widget, subject_widget, map_type_widget, task_widget, source_hemi_widget]),\n",
    "        VBox([cf_property_widget, polar_colormap_widget, r2_threshold_widget, hemi_separation_widget])\n",
    "    ]),\n",
    "    HBox([\n",
    "        adaptive_range_widget,\n",
    "        vmin_widget,\n",
    "        vmax_widget\n",
    "    ]),\n",
    "    out\n",
    "]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b33e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainCoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
